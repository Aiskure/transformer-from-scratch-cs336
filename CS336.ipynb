{"cells":[{"cell_type":"markdown","metadata":{"trusted":true,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D89B0EAB542642F98900CCC4D4E49360","runtime":{"status":"default","execution_status":null,"is_visible":false},"scrolled":false,"notebookId":"688ef4fd01e7980f7a117804"},"source":"# æ–¯å¦ç¦å¤§å­¦CS336ä»é›¶å®ç°å¤§è¯­è¨€æ¨¡å‹è¯¾ç¨‹ä½œä¸šå¤ç°â€”â€”åˆ†è¯ (Tokenization)  \n\n## 1.è¯¾ç¨‹ä½œä¸šæ•´ä½“ä»‹ç»  \nåŒ…æ‹¬è¯¾ç¨‹å†…å®¹å’Œä½œä¸šå®ç°ã€‚  \nä»¥ä¸‹æ˜¯è¯¥ä½œä¸šè¦æ±‚çš„åˆ†ç‚¹è¯´æ˜ï¼š  \n\nç¯å¢ƒåŠèµ„æºï¼š  \n2.5~2.6éƒ¨åˆ†å»ºè®®ä½¿ç”¨è…¾è®¯äº‘å—äº¬ CPU 16æ ¸64G  \nåŸºç¡€é•œåƒï¼ˆ2025ç‰ˆï¼‰  \n\n**ä½ å°†å®ç°çš„å†…å®¹ï¼š**  \n\n1. **å­—èŠ‚å¯¹ç¼–ç ï¼ˆBPEï¼‰åˆ†è¯å™¨**  \n   - ä»å¤´å®ç° BPE ç®—æ³•ï¼Œç”¨äºæ„å»ºå­è¯ï¼ˆsubwordï¼‰çº§åˆ«çš„åˆ†è¯å™¨ã€‚  \n   - åœ¨ TinyStories æ•°æ®é›†ä¸Šè®­ç»ƒè¯¥åˆ†è¯å™¨ã€‚  \n   - ä½¿ç”¨è®­ç»ƒå¥½çš„åˆ†è¯å™¨å°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•´æ•° ID åºåˆ—ï¼Œä¾›æ¨¡å‹è®­ç»ƒä½¿ç”¨ã€‚  \n\n2. **Transformer è¯­è¨€æ¨¡å‹**  \n   - å®Œå…¨ä»å¤´æ„å»ºä¸€ä¸ªæ ‡å‡†çš„ Transformer æ¶æ„è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPT é£æ ¼ï¼‰ã€‚  \n   - åŒ…æ‹¬å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆMulti-Head Self-Attentionï¼‰ã€å‰é¦ˆç½‘ç»œï¼ˆFeed-Forward Networkï¼‰ã€å±‚å½’ä¸€åŒ–ï¼ˆLayer Normalizationï¼‰ã€ä½ç½®ç¼–ç ï¼ˆPositional Encodingï¼‰ç­‰ç»„ä»¶ã€‚  \n   - ä¸å…è®¸ä½¿ç”¨ `torch.nn` ä¸­çš„å±‚å®šä¹‰ï¼ˆå¦‚ Linearã€Embeddingã€LayerNorm ç­‰ï¼‰ï¼Œä½†å¯ä»¥ä½¿ç”¨ `torch.nn.Parameter` å’Œå®¹å™¨ç±»ï¼ˆå¦‚ Moduleã€ModuleListã€Sequentialï¼‰ã€‚  \n\n3. **äº¤å‰ç†µæŸå¤±å‡½æ•°ä¸ AdamW ä¼˜åŒ–å™¨**  \n   - æ‰‹åŠ¨å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆCross-Entropy Lossï¼‰ï¼Œç”¨äºè¯­è¨€æ¨¡å‹è®­ç»ƒã€‚  \n   - ä»å¤´å®ç° AdamW ä¼˜åŒ–å™¨ï¼ˆåŒ…æ‹¬åŠ¨é‡ã€è‡ªé€‚åº”å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡æœºåˆ¶ï¼‰ï¼Œä¸èƒ½ä½¿ç”¨ `torch.optim` ä¸­çš„ç°æˆä¼˜åŒ–å™¨ï¼ˆä½†å¯ç»§æ‰¿ `torch.optim.Optimizer` åŸºç±»ï¼‰ã€‚  \n\n4. **æ”¯æŒåºåˆ—åŒ–/åŠ è½½çš„è®­ç»ƒå¾ªç¯**  \n   - å®ç°å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼Œæ”¯æŒï¼š  \n     - æ¨¡å‹çŠ¶æ€ä¿å­˜ä¸åŠ è½½ï¼ˆ`state_dict` çš„ä¿å­˜ä¸æ¢å¤ï¼‰  \n     - ä¼˜åŒ–å™¨çŠ¶æ€çš„ä¿å­˜ä¸åŠ è½½  \n     - æ–­ç‚¹ç»­è®­åŠŸèƒ½  \n   - è®­ç»ƒè¿‡ç¨‹ä¸­è®°å½•æŸå¤±ã€å›°æƒ‘åº¦ï¼ˆperplexityï¼‰ç­‰æŒ‡æ ‡ã€‚  \n\n---  \n\n**ä½ å°†è¿è¡Œçš„ä»»åŠ¡ï¼š**  \n\n1. **åœ¨ TinyStories æ•°æ®é›†ä¸Šè®­ç»ƒ BPE åˆ†è¯å™¨**  \n   - ä½¿ç”¨æä¾›çš„ TinyStories æ–‡æœ¬æ•°æ®è®­ç»ƒä½ å®ç°çš„ BPE åˆ†è¯å™¨ã€‚  \n\n2. **å°†æ•°æ®é›†è½¬æ¢ä¸ºæ•´æ•° ID åºåˆ—**  \n   - ä½¿ç”¨è®­ç»ƒå¥½çš„ BPE åˆ†è¯å™¨å¯¹ TinyStories æ•°æ®è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆæ¨¡å‹å¯å¤„ç†çš„æ•´æ•° token ID åºåˆ—ã€‚  \n\n3. **åœ¨ TinyStories ä¸Šè®­ç»ƒ Transformer è¯­è¨€æ¨¡å‹**  \n   - ä½¿ç”¨ç¼–ç åçš„æ•°æ®è®­ç»ƒä½ å®ç°çš„ Transformer æ¨¡å‹ã€‚  \n   - è®­ç»ƒæœŸé—´ç›‘æ§è®­ç»ƒæŸå¤±ï¼Œå¹¶åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å›°æƒ‘åº¦ã€‚  \n\n4. **ç”Ÿæˆæ ·æœ¬ä¸è¯„ä¼°å›°æƒ‘åº¦**  \n   - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚é€šè¿‡è‡ªå›å½’é‡‡æ ·ï¼‰ã€‚  \n   - åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å¹¶æŠ¥å‘Šè¯­è¨€æ¨¡å‹çš„å›°æƒ‘åº¦ï¼ˆPerplexityï¼‰ã€‚  \n\n5. **åœ¨ OpenWebText æ•°æ®é›†ä¸Šè®­ç»ƒæ¨¡å‹å¹¶æäº¤ç»“æœ**  \n   - å°†æ¨¡å‹åº”ç”¨äºæ›´å¤§çš„ OpenWebText æ•°æ®é›†è¿›è¡Œè®­ç»ƒã€‚  \n   - åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹çš„å›°æƒ‘åº¦ï¼Œå¹¶å°†ç»“æœæäº¤è‡³è¯¾ç¨‹æŒ‡å®šçš„æ’è¡Œæ¦œã€‚  \n\n---  \n\n**ä½ å¯ä»¥ä½¿ç”¨çš„å·¥å…·ä¸é™åˆ¶ï¼š**  \n\n- **ç¦æ­¢ä½¿ç”¨**ï¼š  \n  - `torch.nn` ä¸­çš„å±‚ç±»ï¼ˆå¦‚ `nn.Linear`, `nn.Embedding`, `nn.LayerNorm`, `nn.Dropout` ç­‰ï¼‰  \n  - `torch.nn.functional` ä¸­çš„å‡½æ•°ï¼ˆå¦‚ `F.linear`, `F.softmax`, `F.cross_entropy` ç­‰ï¼‰  \n  - `torch.optim` ä¸­çš„å…·ä½“ä¼˜åŒ–å™¨ï¼ˆå¦‚ `Adam`, `SGD` ç­‰ï¼‰  \n\n- **å…è®¸ä½¿ç”¨**ï¼š  \n  - `torch.nn.Parameter`ï¼šç”¨äºå®šä¹‰å¯å­¦ä¹ å‚æ•°  \n  - `torch.nn` ä¸­çš„å®¹å™¨ç±»ï¼šå¦‚ `nn.Module`, `nn.ModuleList`, `nn.Sequential` ç­‰ï¼Œç”¨äºç»„ç»‡æ¨¡å‹ç»“æ„  \n  - `torch.optim.Optimizer` åŸºç±»ï¼šå¯ç”¨äºè‡ªå®šä¹‰ä¼˜åŒ–å™¨çš„å®ç°  \n  - å…¶ä»–æ‰€æœ‰ PyTorch å‡½æ•°ï¼ˆå¦‚å¼ é‡æ“ä½œã€è‡ªåŠ¨æ±‚å¯¼ã€æ•°æ®åŠ è½½ç­‰ï¼‰å‡å¯è‡ªç”±ä½¿ç”¨  \n\n- **åŸºæœ¬åŸåˆ™**ï¼š**å¿…é¡»ä»å¤´å®ç°æ ¸å¿ƒç»„ä»¶ï¼Œä¸å¾—ä¾èµ–ç°æˆçš„ç¥ç»ç½‘ç»œæ¨¡å—æˆ–ä¼˜åŒ–å™¨å®ç°**ã€‚  \n\n---  \n\n**å…³äº AI å·¥å…·çš„ä½¿ç”¨å£°æ˜ï¼š**  \n\n- **å…è®¸ä½¿ç”¨**ï¼š  \n  - ChatGPTã€Claude ç­‰å¤§è¯­è¨€æ¨¡å‹æ¥å¸®åŠ©ç†è§£ç¼–ç¨‹é—®é¢˜æˆ–è¯­è¨€æ¨¡å‹çš„åŸºæœ¬æ¦‚å¿µã€‚  \n  - ç”¨äºå­¦ä¹ åŸç†ã€è°ƒè¯•æ€è·¯ã€è§£é‡Šé”™è¯¯ä¿¡æ¯ç­‰ã€‚  \n\n- **ç¦æ­¢ä½¿ç”¨**ï¼š  \n  - ç›´æ¥ç”Ÿæˆä½œä¸šç­”æ¡ˆæˆ–æ ¸å¿ƒä»£ç å®ç°ã€‚  \n  - å°†é—®é¢˜å®Œæ•´è¾“å…¥ AI å¹¶å¤åˆ¶å…¶è¿”å›çš„è§£å†³æ–¹æ¡ˆã€‚  \n\n- **ç›®çš„æé†’**ï¼š  \n  - æœ¬ä½œä¸šæ—¨åœ¨åŠ æ·±å¯¹ Transformerã€BPEã€ä¼˜åŒ–å™¨ç­‰æ ¸å¿ƒæŠ€æœ¯çš„æ‰‹åŠ¨å®ç°ç†è§£ï¼Œè¿‡åº¦ä¾èµ– AI å°†å‰Šå¼±å­¦ä¹ æ•ˆæœã€‚  \n\n---  \n\nè¯·æ ¹æ®ä»¥ä¸Šè¦æ±‚ï¼Œé€æ­¥å®Œæˆå„æ¨¡å—çš„å®ç°ä¸è®­ç»ƒä»»åŠ¡ã€‚  \n"},{"cell_type":"markdown","metadata":{"id":"3844E1912FB4423A9F85EDA198B41108","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.1 Unicodeæ ‡å‡†  \nåœ¨æœ¬ä½œä¸šçš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†å®ç°å¹¶è®­ç»ƒä¸€ä¸ª**å­—èŠ‚çº§å­—èŠ‚å¯¹ç¼–ç ï¼ˆByte-Pair Encoding, BPEï¼‰åˆ†è¯å™¨**ï¼Œè¯¥æ–¹æ³•åŸºäº Sennrich ç­‰äººï¼ˆ2016ï¼‰å’Œ Wang ç­‰äººï¼ˆ2019ï¼‰çš„å·¥ä½œã€‚ä¸ä¼ ç»Ÿçš„åŸºäºå­—ç¬¦æˆ–å­è¯çš„ BPE ä¸åŒï¼Œæˆ‘ä»¬å°†ä»¥**å­—èŠ‚ï¼ˆbyteï¼‰**ä¸ºåŸºæœ¬å•ä½è¿›è¡Œåˆ†è¯å™¨çš„æ„å»ºã€‚  \n\nå…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ä¼šå°†ä»»æ„ Unicode å­—ç¬¦ä¸²é¦–å…ˆè½¬æ¢ä¸ºå¯¹åº”çš„å­—èŠ‚åºåˆ—ï¼ˆä½¿ç”¨ UTF-8 ç¼–ç ï¼‰ï¼Œç„¶ååœ¨è¿™ä¸ªå­—èŠ‚åºåˆ—ä¸Šè¿è¡Œ BPE ç®—æ³•ã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼š  \n- å¯ä»¥è‡ªç„¶åœ°å¤„ç†æ‰€æœ‰ Unicode å­—ç¬¦ï¼ˆåŒ…æ‹¬è‹±æ–‡ã€ä¸­æ–‡ã€è¡¨æƒ…ç¬¦å·ç­‰ï¼‰ï¼Œæ— éœ€æ˜¾å¼åœ°ç»´æŠ¤åºå¤§çš„è¯æ±‡è¡¨ï¼›  \n- æ¨¡å‹å…·å¤‡å¼€ç®±å³ç”¨çš„å¤šè¯­è¨€æ”¯æŒèƒ½åŠ›ï¼›  \n- é¿å…äº†æœªçŸ¥è¯ï¼ˆOOV, out-of-vocabularyï¼‰é—®é¢˜ï¼Œå› ä¸ºä»»ä½•å­—ç¬¦ä¸²éƒ½å¯ä»¥è¢«ç¼–ç ä¸ºå­—èŠ‚åºåˆ—ã€‚  \n\næœ€ç»ˆï¼Œè¯¥åˆ†è¯å™¨èƒ½å¤Ÿå°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ•´æ•° token ID åºåˆ—ï¼Œä¾›åç»­çš„ Transformer è¯­è¨€æ¨¡å‹è®­ç»ƒä½¿ç”¨ã€‚  \n\n#### Unicode ä¸å­—èŠ‚ç¼–ç ç®€ä»‹  \n\nåœ¨ Python ä¸­ï¼Œå­—ç¬¦ä¸²ä»¥ Unicode å½¢å¼å­˜å‚¨ã€‚æ¯ä¸ªå­—ç¬¦å¯¹åº”ä¸€ä¸ªå”¯ä¸€çš„ç ç‚¹ï¼ˆcode pointï¼‰ï¼Œå¯ä»¥é€šè¿‡ `ord()` å‡½æ•°è·å–å…¶æ•´æ•°å€¼ï¼Œä¾‹å¦‚ï¼š  \n```python  \nord('s')   # è¾“å‡º: 115  \nord('ç‰›')  # è¾“å‡º: 29275  \n```\nåä¹‹ï¼Œä½¿ç”¨ `chr()` å¯ä»¥å°†ç ç‚¹è½¬æ¢å›å­—ç¬¦ï¼š  \n```python  \nchr(115)    # è¾“å‡º: 's'  \nchr(29275)  # è¾“å‡º: 'ç‰›'  \n```\n\nä¸ºäº†è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ UTF-8 ç¼–ç ï¼š  \n```python  \n\"Hello ğŸ˜‚\".encode('utf-8')  # è¾“å‡º: b'Hello \\xf0\\x9f\\x98\\x82'  \n```\nè¿™ä¼šè¿”å›ä¸€ä¸ªå­—èŠ‚åºåˆ—ï¼ˆ`bytes` ç±»å‹ï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ 0 åˆ° 255 ä¹‹é—´çš„æ•´æ•°ã€‚BPE åˆ†è¯å™¨å°†åœ¨è¿™æ ·çš„å­—èŠ‚åºåˆ—ä¸Šè¿›è¡Œè®­ç»ƒã€‚  \n"},{"cell_type":"code","metadata":{"id":"AE974F7B27574A0DBB1BE0E8C2195B9F","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"print(ord('ç‰›'))\n\nchr(29275)\n# chr(0x1F60E)","outputs":[{"output_type":"stream","name":"stdout","text":"29275\n"},{"output_type":"execute_result","data":{"text/plain":"'ç‰›'"},"metadata":{},"execution_count":11}],"execution_count":11},{"cell_type":"markdown","metadata":{"id":"C3111AA3578944698B6888D02424E0F9","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### é—®é¢˜ (1 åˆ†)**  \n(a) chr(0)è¿”å›ä»€ä¹ˆUnicodeå­—ç¬¦ï¼Ÿ  \n(b) è¯¥å­—ç¬¦çš„å­—ç¬¦ä¸²è¡¨ç¤ºä¸æ‰“å°è¡¨ç¤ºæœ‰ä½•ä¸åŒï¼Ÿ  \n(c) å½“è¯¥å­—ç¬¦å‡ºç°åœ¨æ–‡æœ¬ä¸­ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ"},{"cell_type":"code","metadata":{"id":"6223A97595184617B887266CF0DE9B40","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"chr(0)","outputs":[{"output_type":"execute_result","data":{"text/plain":"'\\x00'"},"metadata":{},"execution_count":19}],"execution_count":19},{"cell_type":"code","metadata":{"id":"1CFC0F195B3B463A970AFFB42687EBC1","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"print(chr(0))","outputs":[{"output_type":"stream","name":"stdout","text":"\u0000\n"}],"execution_count":20},{"cell_type":"code","metadata":{"id":"902ED1BD38CD43B9B5DC8B5E59475899","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"l = \"this is a test\" + chr(0) + \"string\"\nl","outputs":[{"output_type":"execute_result","data":{"text/plain":"'this is a test\\x00string'"},"metadata":{},"execution_count":21}],"execution_count":21},{"cell_type":"code","metadata":{"id":"062F4AA792F744CAAF9C7085527369C4","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":" print(\"this is a test\" + chr(0) + \"string\")","outputs":[{"output_type":"stream","name":"stdout","text":"this is a test\u0000string\n"}],"execution_count":22},{"cell_type":"markdown","metadata":{"id":"6E780BDE52B8473B9DC84F614AA1040E","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.2 Unicodeç¼–ç   \nå°½ç®¡ Unicode æ ‡å‡†ä¸ºæ¯ä¸ªå­—ç¬¦å®šä¹‰äº†å”¯ä¸€çš„ä»£ç ç‚¹ï¼ˆå³ä¸€ä¸ªæ•´æ•°ï¼‰ï¼Œä½†ç›´æ¥åœ¨ Unicode ä»£ç ç‚¹ä¸Šè®­ç»ƒåˆ†è¯å™¨å¹¶ä¸å¯è¡Œã€‚ä¸»è¦åŸå› æœ‰ä¸¤ä¸ªï¼šä¸€æ˜¯ Unicode å­—ç¬¦æ€»æ•°åºå¤§ï¼ˆç›®å‰å·²å®šä¹‰çš„å­—ç¬¦è¶…è¿‡ 15 ä¸‡ä¸ªï¼‰ï¼Œå¯¼è‡´è¯æ±‡è¡¨è§„æ¨¡è¿‡å¤§ï¼›äºŒæ˜¯å¤§å¤šæ•°å­—ç¬¦åœ¨å®é™…æ–‡æœ¬ä¸­æä¸ºç½•è§ï¼Œé€ æˆè¯æ±‡è¡¨é«˜åº¦ç¨€ç–ï¼Œä¸åˆ©äºæ¨¡å‹å­¦ä¹ å’Œæ³›åŒ–ã€‚  \n\nä¸ºè§£å†³è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬è½¬è€Œä½¿ç”¨ Unicode ç¼–ç æ–¹æ¡ˆ å°†æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—ï¼Œå¹¶åœ¨å­—èŠ‚çº§åˆ«ä¸Šæ„å»ºåˆ†è¯å™¨ã€‚Unicode å®šä¹‰äº†å¤šç§ç¼–ç æ ¼å¼ï¼Œå…¶ä¸­æœ€å¸¸ç”¨çš„æ˜¯ UTF-8ã€UTF-16 å’Œ UTF-32ã€‚åœ¨è¿™äº›ç¼–ç ä¸­ï¼ŒUTF-8 æ˜¯å½“å‰äº’è”ç½‘ä¸Šæœ€ä¸»æµçš„ç¼–ç æ–¹å¼ï¼Œæ®ä¼°è®¡è¶…è¿‡ 98% çš„ç½‘é¡µéƒ½é‡‡ç”¨ UTF-8ã€‚  \n\nUTF-8 çš„ä¸€ä¸ªé‡è¦ç‰¹æ€§æ˜¯ï¼šå®ƒå°†æ¯ä¸ª Unicode å­—ç¬¦ç¼–ç ä¸º 1 åˆ° 4 ä¸ªå­—èŠ‚çš„åºåˆ—ï¼ˆå¯¹äºåŸºæœ¬ ASCII å­—ç¬¦ä»…ç”¨ 1 å­—èŠ‚ï¼Œè€Œä¸­æ–‡ã€è¡¨æƒ…ç¬¦å·ç­‰åˆ™ä½¿ç”¨ 3 æˆ– 4 å­—èŠ‚ï¼‰ï¼Œå…¼å®¹ ASCII ä¸”å¯å˜é•¿ï¼Œé«˜æ•ˆä¸”å¹¿æ³›æ”¯æŒã€‚"},{"cell_type":"code","metadata":{"id":"120383D82403486D95115C1F419D8506","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# åŠ¨æ‰‹è·‘ï¼\ntest_string = \"hello! å¤©æµ·!\"  # è¯•è¯•ä½ çš„åå­—ï¼Ÿ\nutf8_encoded = test_string.encode(\"utf-8\")\nprint(utf8_encoded)  # è¾“å‡º: b'hello! \\xe5\\xa4\\xa9\\xe6\\xb5\\xb7!'\nprint(type(utf8_encoded)) \n\n# æ‹†è§£å­—èŠ‚å€¼ï¼ˆ0-255æ•´æ•°ï¼‰\nlist(utf8_encoded)  \n\n# éªŒè¯å¯é€†æ€§\nprint(len(test_string), len(utf8_encoded))  \nprint(utf8_encoded.decode(\"utf-8\"))  # å®Œç¾è¿˜åŸ: 'hello! å¤©æµ·!'","outputs":[{"output_type":"stream","name":"stdout","text":"b'hello! \\xe5\\xa4\\xa9\\xe6\\xb5\\xb7!'\n<class 'bytes'>\n10 14\nhello! å¤©æµ·!\n"}],"execution_count":25},{"cell_type":"markdown","metadata":{"id":"9715B9B4F8994339AAB208B073B50852","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### é—®é¢˜ (3 åˆ†)  \n(a) ä¸ºä»€ä¹ˆæˆ‘ä»¬æ›´å€¾å‘äºåœ¨UTF-8ç¼–ç çš„å­—èŠ‚ä¸Šè®­ç»ƒåˆ†è¯å™¨ï¼Œè€Œä¸æ˜¯UTF-16æˆ–UTF-32ï¼Ÿæ¯”è¾ƒè¿™äº›ç¼–ç å¯¹ä¸åŒè¾“å…¥å­—ç¬¦ä¸²çš„è¾“å‡ºå¯èƒ½æœ‰æ‰€å¸®åŠ©ã€‚  \näº¤ä»˜è¦æ±‚ï¼šä¸€è‡³ä¸¤å¥è¯çš„å›ç­”ã€‚  \n\n(b) è€ƒè™‘ä»¥ä¸‹ï¼ˆé”™è¯¯çš„ï¼‰å‡½æ•°ï¼Œå…¶ç›®çš„æ˜¯å°†UTF-8å­—èŠ‚ä¸²è§£ç ä¸ºUnicodeå­—ç¬¦ä¸²ã€‚ä¸ºä»€ä¹ˆè¿™ä¸ªå‡½æ•°æ˜¯é”™è¯¯çš„ï¼Ÿæä¾›ä¸€ä¸ªä¼šäº§ç”Ÿé”™è¯¯ç»“æœçš„è¾“å…¥å­—èŠ‚ä¸²ç¤ºä¾‹ã€‚  \n\n\n```python  \ndef decode_utf8_bytes_to_str_wrong(bytestring: bytes):  # æ³¨æ„å‚æ•°åä¿®æ­£ï¼  \n    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])  \n```\n> *è¯•è¯• `decode_utf8_bytes_to_str_wrong(\"cafÃ©\".encode(\"utf-8\"))` â€” å´©äº†æ²¡ï¼Ÿ*  "},{"cell_type":"code","metadata":{"id":"D3A6E1847429425E9FBE94E417340FC5","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"def decode_utf8_bytes_to_str_wrong(bytestring: bytes):  # æ³¨æ„å‚æ•°åä¿®æ­£ï¼\n    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\ndecode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))\n# decode_utf8_bytes_to_str_wrong(\"cafÃ©\".encode(\"utf-8\")) é”™è¯¯","outputs":[{"output_type":"execute_result","data":{"text/plain":"'hello'"},"metadata":{},"execution_count":29}],"execution_count":29},{"cell_type":"markdown","metadata":{"id":"8E020D9D0D41423A9DA5E976A5A59FB7","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.3 å­å­—æ ‡è®°åŒ–ï¼ˆsubword tokenizationï¼‰  \nè™½ç„¶å­—èŠ‚çº§æ ‡è®°åŒ–èƒ½å¤Ÿæœ‰æ•ˆç¼“è§£å•è¯çº§æ ‡è®°å™¨é¢ä¸´çš„è¯æ±‡è¡¨å¤–é—®é¢˜ï¼Œä½†å°†æ–‡æœ¬åˆ†è§£ä¸ºå•ä¸ªå­—èŠ‚ä¼šå¯¼è‡´è¾“å…¥åºåˆ—è¿‡é•¿ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªåŒ…å«10ä¸ªå•è¯çš„å¥å­åœ¨å•è¯çº§æ¨¡å‹ä¸­å¯èƒ½ä»…å¯¹åº”10ä¸ªæ ‡è®°ï¼Œè€Œåœ¨å­—èŠ‚çº§æ¨¡å‹ä¸­å´å¯èƒ½è†¨èƒ€è‡³50ä¸ªç”šè‡³æ›´å¤šæ ‡è®°ï¼Œå…·ä½“å–å†³äºå•è¯é•¿åº¦ã€‚è¿™ç§æ‰©å±•æ˜¾è‘—å¢åŠ äº†æ¨¡å‹æ¯ä¸€æ­¥çš„è®¡ç®—é‡ï¼Œæ‹–æ…¢è®­ç»ƒé€Ÿåº¦ã€‚åŒæ—¶ï¼Œè¿‡é•¿çš„åºåˆ—ä¹Ÿç»™è¯­è¨€å»ºæ¨¡å¸¦æ¥æŒ‘æˆ˜ï¼Œå› ä¸ºå®ƒåœ¨æ•°æ®ä¸­å¼•å…¥äº†æ›´å¤æ‚çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚  \n\nå­å­—æ ‡è®°åŒ–ï¼ˆsubword tokenizationï¼‰åˆ™ä»‹äºå•è¯çº§å’Œå­—èŠ‚çº§ä¹‹é—´ï¼Œæä¾›äº†ä¸€ç§æŠ˜ä¸­æ–¹æ¡ˆã€‚ä¸ä»…æœ‰256ä¸ªæ¡ç›®çš„å­—èŠ‚çº§è¯æ±‡è¡¨ä¸åŒï¼Œå­è¯æ ‡è®°å™¨é€šè¿‡æ‰©å¤§è¯æ±‡é‡æ¥æ›´é«˜æ•ˆåœ°å‹ç¼©åŸå§‹å­—èŠ‚åºåˆ—ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå¦‚æœæŸäº›å­—èŠ‚åºåˆ—ï¼ˆå¦‚ `b'the'`ï¼‰åœ¨è®­ç»ƒæ•°æ®ä¸­é¢‘ç¹å‡ºç°ï¼Œå°±å°†å…¶åˆå¹¶ä¸ºä¸€ä¸ªå•ç‹¬çš„æ ‡è®°ï¼Œä»è€Œå°†åŸæœ¬å¤šä¸ªå­—èŠ‚ç»„æˆçš„åºåˆ—å‹ç¼©ä¸ºä¸€ä¸ªå•å…ƒã€‚è¿™æ ·æ—¢èƒ½ä¿æŒå¯¹ç½•è§è¯å’ŒæœªçŸ¥è¯çš„å¤„ç†èƒ½åŠ›ï¼Œåˆèƒ½æ˜¾è‘—ç¼©çŸ­å¹³å‡åºåˆ—é•¿åº¦ã€‚  \n\nå¦‚ä½•é€‰æ‹©è¿™äº›å­è¯å•å…ƒï¼ŸSennrich ç­‰äººï¼ˆ2016ï¼‰æå‡ºé‡‡ç”¨å­—èŠ‚å¯¹ç¼–ç ï¼ˆByte Pair Encoding, BPEï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æºè‡ªæ•°æ®å‹ç¼©æŠ€æœ¯çš„ç®—æ³•ã€‚BPE é€šè¿‡è¿­ä»£åœ°æŸ¥æ‰¾å¹¶åˆå¹¶å‡ºç°é¢‘ç‡æœ€é«˜çš„ç›¸é‚»å­—èŠ‚å¯¹ï¼Œé€æ­¥æ„å»ºå‡ºä¸€ç»„é«˜æ•ˆçš„å­è¯å•å…ƒã€‚æ¯æ¬¡åˆå¹¶éƒ½ä¼šå¼•å…¥ä¸€ä¸ªæ–°çš„ç¬¦å·æ¥ä»£è¡¨è¯¥å­—èŠ‚å¯¹ï¼Œå¹¶å°†å…¶åŠ å…¥è¯æ±‡è¡¨ã€‚è¿™ä¸€è¿‡ç¨‹æŒç»­è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾çš„è¯æ±‡è¡¨å¤§å°ã€‚ç”±äºBPEä¼˜å…ˆåˆå¹¶é«˜é¢‘æ¨¡å¼ï¼Œå› æ­¤æœ€ç»ˆçš„è¯æ±‡è¡¨èƒ½æœ€å¤§ç¨‹åº¦åœ°æå‡æ•´ä½“å‹ç¼©æ•ˆç‡â€”â€”å¸¸è§è¯æˆ–è¯ç‰‡æ®µæ›´å¯èƒ½è¢«è¡¨ç¤ºä¸ºå•ä¸€æ ‡è®°ã€‚  \n\nåœ¨æœ¬ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ç§åŸºäºå­—èŠ‚çš„BPEåˆ†è¯å™¨ï¼Œå…¶è¯æ±‡é¡¹ç”±åŸå§‹å­—èŠ‚åŠå…¶åˆå¹¶åçš„åºåˆ—è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•ç»“åˆäº†å­—èŠ‚çº§åˆ†è¯å™¨çš„é²æ£’æ€§ä¸å­è¯çº§çš„é«˜æ•ˆæ€§ï¼Œåœ¨å¤„ç†æœªç™»å½•è¯çš„åŒæ—¶ä¿æŒåˆç†çš„åºåˆ—é•¿åº¦ã€‚æ•´ä¸ªæ„å»ºè¯æ±‡è¡¨çš„è¿‡ç¨‹ä¹Ÿè¢«ç§°ä¸ºâ€œè®­ç»ƒâ€BPEåˆ†è¯å™¨ï¼Œæ˜¯å®ç°é«˜æ•ˆæ–‡æœ¬è¡¨ç¤ºçš„å…³é”®æ­¥éª¤ã€‚"},{"cell_type":"markdown","metadata":{"id":"9725FBB1BDD34C5F82E645E3A23E17BF","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.4 BPE åˆ†è¯å™¨è®­ç»ƒ  \n\nBPEåˆ†è¯å™¨çš„è®­ç»ƒåŒ…å«ä¸‰ä¸ªæ­¥éª¤ï¼šåˆå§‹åŒ–ã€é¢„åˆ†è¯å’Œåˆå¹¶ã€‚  \n\n### 2.4.1 è¯è¡¨åˆå§‹åŒ–  \né¦–å…ˆè¿›è¡Œè¯æ±‡è¡¨çš„åˆå§‹åŒ–ï¼ŒBPEåˆ†è¯å™¨çš„è¯æ±‡è¡¨æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªä»å­—èŠ‚å­—ç¬¦ä¸²åˆ°æ•´æ•°IDçš„ä¸€ä¸€æ˜ å°„ã€‚ç”±äºæˆ‘ä»¬è®­ç»ƒçš„æ˜¯å­—èŠ‚çº§BPEåˆ†è¯å™¨ï¼Œåˆå§‹è¯æ±‡è¡¨åŒ…å«æ‰€æœ‰256ä¸ªå¯èƒ½çš„å­—èŠ‚å€¼ï¼Œå› æ­¤åˆå§‹å¤§å°ä¸º256ã€‚  \n\n\n### 2.4.2 é¢„åˆ†è¯  \n\næ¥ä¸‹æ¥æ˜¯é¢„åˆ†è¯é˜¶æ®µã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨åŸå§‹å­—èŠ‚åºåˆ—ä¸Šç»Ÿè®¡ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡å¹¶å¼€å§‹åˆå¹¶ï¼Œä½†è¿™æ ·æ¯æ¬¡åˆå¹¶éƒ½éœ€è¦éå†æ•´ä¸ªè¯­æ–™åº“ï¼Œè®¡ç®—æˆæœ¬æé«˜ã€‚æ­¤å¤–ï¼Œè‹¥ä¸åŠ å¤„ç†åœ°è·¨æ–‡æœ¬åˆå¹¶å­—èŠ‚ï¼Œå¯èƒ½å¯¼è‡´ä»…å› æ ‡ç‚¹ä¸åŒè€Œè¯­ä¹‰ç›¸è¿‘çš„è¯è¢«æ‹†åˆ†ä¸ºå®Œå…¨ä¸åŒçš„ä¸€ç»„æ ‡è®°ï¼Œä¾‹å¦‚â€œdog!â€å’Œâ€œdog.â€ä¼šè¢«è§†ä¸ºå®Œå…¨æ— å…³çš„æ ‡è®°ï¼Œä¸åˆ©äºæ¨¡å‹å­¦ä¹ å…¶è¯­ä¹‰ä¸€è‡´æ€§ã€‚ä¸ºç¼“è§£è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å…ˆå¯¹è¯­æ–™è¿›è¡Œé¢„åˆ†è¯ã€‚é¢„åˆ†è¯å¯ä»¥çœ‹ä½œæ˜¯å¯¹æ–‡æœ¬çš„ä¸€æ¬¡ç²—ç²’åº¦åˆ‡åˆ†ï¼Œæœ‰åŠ©äºé«˜æ•ˆç»Ÿè®¡å­—èŠ‚å¯¹çš„å…±ç°é¢‘ç‡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå•è¯â€œtextâ€ä½œä¸ºé¢„åˆ†è¯å‡ºç°äº†10æ¬¡ï¼Œé‚£ä¹ˆå…¶ä¸­çš„å­—èŠ‚å¯¹â€˜tâ€™å’Œâ€˜eâ€™çš„å…±ç°æ¬¡æ•°å°±å¯ä»¥ä¸€æ¬¡æ€§å¢åŠ 10æ¬¡ï¼Œè€Œæ— éœ€é‡å¤æ‰«ææ•´ä¸ªè¯­æ–™ã€‚æ¯ä¸ªé¢„åˆ†è¯ä»¥UTF-8ç¼–ç çš„å­—èŠ‚åºåˆ—è¡¨ç¤ºã€‚  \n\nåŸå§‹BPEå®ç°ä¸­é‡‡ç”¨ç®€å•çš„ç©ºæ ¼åˆ†å‰²ï¼ˆs.split(\" \")ï¼‰ï¼Œè€Œæˆ‘ä»¬åˆ™é‡‡ç”¨GPT-2æ‰€ä½¿ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼é¢„åˆ†è¯å™¨ï¼ˆæ¥è‡ªOpenAIçš„tiktokené¡¹ç›®ï¼‰ï¼Œå…¶æ¨¡å¼ä¸ºï¼š`r\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"`ã€‚ä¾‹å¦‚å¯¹å¥å­â€œsome text that i'll pre-tokenizeâ€è¿›è¡Œåˆ‡åˆ†ï¼Œç»“æœä¸ºï¼š['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']ã€‚åœ¨å®é™…ç¼–ç ä¸­ï¼Œå»ºè®®ä½¿ç”¨`re.finditer`è€Œé`re.findall`ï¼Œä»¥ä¾¿åœ¨æ„å»ºé¢„åˆ†è¯é¢‘æ¬¡æ˜ å°„æ—¶é¿å…é¢å¤–å­˜å‚¨ä¸­é—´åˆ—è¡¨ã€‚  \n\n### 2.4.3 BPEåˆå¹¶è®¡ç®—  \n\nå®Œæˆé¢„åˆ†è¯åï¼Œè¿›å…¥BPEåˆå¹¶çš„è®¡ç®—é˜¶æ®µã€‚æ­¤æ—¶ï¼Œæ¯ä¸ªé¢„åˆ†è¯å·²è¢«è¡¨ç¤ºä¸ºå­—èŠ‚åºåˆ—ï¼Œç®—æ³•å¼€å§‹è¿­ä»£åœ°ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°é¢‘ç‡ï¼Œå¹¶é€‰æ‹©é¢‘ç‡æœ€é«˜çš„å­—èŠ‚å¯¹è¿›è¡Œåˆå¹¶ã€‚ä¾‹å¦‚ï¼Œè‹¥å­—èŠ‚å¯¹ï¼ˆA, Bï¼‰æ˜¯å½“å‰æœ€é¢‘ç¹çš„ç»„åˆï¼Œåˆ™å°†æ‰€æœ‰è¿ç»­å‡ºç°çš„Aå’ŒBæ›¿æ¢ä¸ºä¸€ä¸ªæ–°çš„åˆå¹¶æ ‡è®°â€œABâ€ï¼Œå¹¶å°†è¯¥æ–°æ ‡è®°åŠ å…¥è¯æ±‡è¡¨ã€‚ç”±äºåˆå§‹è¯æ±‡è¡¨å¤§å°ä¸º256ï¼Œæœ€ç»ˆè¯æ±‡è¡¨çš„å¤§å°ç­‰äº256åŠ ä¸Šè®­ç»ƒè¿‡ç¨‹ä¸­æ‰§è¡Œçš„åˆå¹¶æ“ä½œæ•°é‡ã€‚ä¸ºäº†æå‡æ•ˆç‡ï¼Œåœ¨ç»Ÿè®¡å­—èŠ‚å¯¹æ—¶ä¸ä¼šè·¨è¶Šé¢„åˆ†è¯è¾¹ç•Œè¿›è¡Œåˆå¹¶ã€‚å½“å¤šä¸ªå­—èŠ‚å¯¹é¢‘ç‡ç›¸åŒæ—¶ï¼Œé‡‡ç”¨å­—å…¸åºè¾ƒå¤§çš„é‚£ä¸€å¯¹ä½œä¸ºèƒœå‡ºè€…ã€‚ä¾‹å¦‚ï¼Œè‹¥ï¼ˆ\"A\", \"B\"ï¼‰ã€ï¼ˆ\"A\", \"C\"ï¼‰ã€ï¼ˆ\"B\", \"ZZ\"ï¼‰å’Œï¼ˆ\"BA\", \"A\"ï¼‰é¢‘ç‡ç›¸åŒï¼Œåˆ™é€‰æ‹©å­—å…¸åºæœ€å¤§çš„ï¼ˆ\"BA\", \"A\"ï¼‰è¿›è¡Œåˆå¹¶ã€‚  \n\næ­¤å¤–ï¼ŒæŸäº›ç‰¹æ®Šå­—ç¬¦ä¸²ï¼ˆå¦‚ ï¼‰ç”¨äºè¡¨ç¤ºå…ƒä¿¡æ¯ï¼Œä¾‹å¦‚æ–‡æ¡£è¾¹ç•Œæˆ–åºåˆ—ç»“æŸã€‚è¿™ç±»å­—ç¬¦ä¸²åº”ä½œä¸ºâ€œç‰¹æ®Šæ ‡è®°â€è¢«æ•´ä½“ä¿ç•™ï¼Œç»ä¸å…è®¸è¢«æ‹†åˆ†ã€‚å› æ­¤ï¼Œè¿™äº›ç‰¹æ®Šæ ‡è®°å¿…é¡»æå‰åŠ å…¥è¯æ±‡è¡¨ï¼Œå¹¶åˆ†é…å›ºå®šçš„IDã€‚ä¾‹å¦‚ï¼Œ è¡¨ç¤ºåºåˆ—ç»“æŸï¼Œå¿…é¡»å§‹ç»ˆå¯¹åº”å•ä¸€æ ‡è®°ï¼Œä»¥ä¾¿æ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶èƒ½å‡†ç¡®åˆ¤æ–­ä½•æ—¶åœæ­¢ã€‚Sennrichç­‰äººï¼ˆ2016ï¼‰çš„åŸå§‹è®ºæ–‡ä¸­ç»™å‡ºäº†BPEè®­ç»ƒçš„éä¼˜åŒ–ç‰ˆæœ¬ï¼Œé€‚åˆåˆå­¦è€…å®ç°ä»¥ç†è§£åŸºæœ¬æµç¨‹ã€‚  \n\nä»¥è®ºæ–‡ä¸­çš„ç¤ºä¾‹è¯´æ˜ï¼šå‡è®¾è¯­æ–™åŒ…å«ä»¥ä¸‹æ–‡æœ¬ï¼š  \n```\nlow low low low low  \nlower lower widest widest widest  \nnewest newest newest newest newest newest  \n```\nå¹¶è®¾å®š<|endoftext|>ä¸ºç‰¹æ®Šæ ‡è®°ã€‚  \n\nåˆå§‹è¯æ±‡è¡¨åŒ…æ‹¬<|endoftext|>å’Œå…¨éƒ¨256ä¸ªå­—èŠ‚ã€‚  \n\né¢„åˆ†è¯é‡‡ç”¨ç©ºæ ¼åˆ†å‰²ï¼Œå¾—åˆ°é¢‘æ¬¡ç»Ÿè®¡ï¼šlowï¼ˆ5æ¬¡ï¼‰ã€lowerï¼ˆ2æ¬¡ï¼‰ã€widestï¼ˆ3æ¬¡ï¼‰ã€newestï¼ˆ6æ¬¡ï¼‰ï¼Œå¯è¡¨ç¤ºä¸ºå­—èŠ‚å…ƒç»„çš„é¢‘æ•°å­—å…¸ï¼Œå¦‚{(b'l', b'o', b'w'): 5, ...}ã€‚éšåç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„é¢‘ç‡ï¼šloï¼ˆ7æ¬¡ï¼‰ã€owï¼ˆ7æ¬¡ï¼‰ã€weï¼ˆ8æ¬¡ï¼‰ã€erï¼ˆ2æ¬¡ï¼‰ã€wiï¼ˆ3æ¬¡ï¼‰ã€idï¼ˆ3æ¬¡ï¼‰ã€deï¼ˆ3æ¬¡ï¼‰ã€esï¼ˆ9æ¬¡ï¼‰ã€stï¼ˆ9æ¬¡ï¼‰ã€neï¼ˆ6æ¬¡ï¼‰ã€ewï¼ˆ6æ¬¡ï¼‰ã€‚å…¶ä¸­ï¼ˆesï¼‰å’Œï¼ˆstï¼‰é¢‘ç‡æœ€é«˜ä¸”ç›¸åŒï¼ŒæŒ‰å­—å…¸åºé€‰æ‹©æ›´å¤§çš„ï¼ˆstï¼‰è¿›è¡Œåˆå¹¶ã€‚äºæ˜¯ï¼Œæ‰€æœ‰åŒ…å«â€œstâ€çš„è¯å¦‚â€œwidestâ€å’Œâ€œnewestâ€ä¸­çš„â€œsâ€å’Œâ€œtâ€è¢«åˆå¹¶ä¸ºæ–°æ ‡è®°â€œstâ€ã€‚ç¬¬äºŒè½®ä¸­ï¼Œâ€œeâ€ä¸â€œstâ€ç»„åˆå‡ºç°9æ¬¡ï¼Œæˆä¸ºæœ€é«˜é¢‘å¯¹ï¼Œåˆå¹¶ä¸ºâ€œestâ€ã€‚ç»§ç»­æ­¤è¿‡ç¨‹ï¼Œåç»­åˆå¹¶ä¾æ¬¡ä¸ºâ€œowâ€ã€â€œlowâ€ã€â€œwestâ€ã€â€œneâ€ç­‰ã€‚è‹¥ä»…æ‰§è¡Œ6æ¬¡åˆå¹¶ï¼Œæœ€ç»ˆæ–°å¢æ ‡è®°ä¸º['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']ã€‚  \n\n[<|endoftext|>, [...256 BYTE CHARS], st, est, ow, low, west, ne]  \n\næ­¤æ—¶ï¼Œâ€œnewestâ€å°†è¢«åˆ‡åˆ†ä¸º[ne, west]ä¸¤ä¸ªæ ‡è®°ã€‚è¿™ä¸€æœºåˆ¶åœ¨ä¿æŒå¯¹æœªçŸ¥è¯å¤„ç†èƒ½åŠ›çš„åŒæ—¶ï¼Œæœ‰æ•ˆå‹ç¼©äº†åºåˆ—é•¿åº¦ï¼Œæå‡äº†æ¨¡å‹æ•ˆç‡ã€‚  \n"},{"cell_type":"code","metadata":{"id":"377A6EAEE8BF4EF994F0E1D3982F57F4","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])","outputs":[{"output_type":"execute_result","data":{"text/plain":"'st'"},"metadata":{},"execution_count":3}],"execution_count":3},{"cell_type":"code","metadata":{"id":"AAD1C93036894BB0A36033AB20B1CC94","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"max([(\"es\"),(\"st\")])","outputs":[],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"0F68AFBB0F004F839AB8B11409DE3DD8","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.5 BPE åˆ†è¯å™¨è®­ç»ƒå®éªŒ  \n\nç°åœ¨æˆ‘ä»¬åœ¨ TinyStories æ•°æ®é›†ä¸Šè®­ç»ƒå­—èŠ‚çº§ BPE åˆ†è¯å™¨ã€‚  \nhttps://huggingface.co/datasets/roneneldan/TinyStories/tree/main  \n\n### å¹¶è¡ŒåŒ–é¢„åˆ†è¯  \n\né¢„åˆ†è¯æ˜¯ä¸»è¦æ€§èƒ½ç“¶é¢ˆï¼Œå¯ä½¿ç”¨ `multiprocessing` åº“è¿›è¡Œå¹¶è¡ŒåŠ é€Ÿã€‚å»ºè®®åœ¨ç¡®ä¿åˆ†å—è¾¹ç•Œä½äºç‰¹æ®Š token èµ·å§‹ä½ç½®çš„å‰æä¸‹å¯¹è¯­æ–™åˆ†å—ã€‚å¯å‚è€ƒä»¥ä¸‹é“¾æ¥ä¸­çš„ç¤ºä¾‹ä»£ç è·å–åˆ†å—è¾¹ç•Œï¼Œç”¨äºè·¨è¿›ç¨‹ä»»åŠ¡åˆ†é…ï¼šhttps://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.pyã€‚  \n\nè¯¥åˆ†å—æ–¹å¼å§‹ç»ˆæœ‰æ•ˆï¼Œå› ä¸ºæˆ‘ä»¬ä¸ä¼šåœ¨æ–‡æ¡£è¾¹ç•Œæ‰§è¡Œåˆå¹¶æ“ä½œã€‚æœ¬ä½œä¸šä¸­å¯å§‹ç»ˆé‡‡ç”¨æ­¤æ–¹æ³•ï¼Œæ— éœ€è€ƒè™‘æç«¯æƒ…å†µï¼ˆå¦‚è¶…å¤§è¯­æ–™ä¸­ä¸å« æ ‡è®°çš„æƒ…å½¢ï¼‰ã€‚  \n\n### é¢„åˆ†è¯å‰ç§»é™¤ç‰¹æ®Š token  \n\nåœ¨ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼ˆ`re.finditer`ï¼‰è¿›è¡Œé¢„åˆ†è¯å‰ï¼Œåº”å…ˆä»è¯­æ–™ï¼ˆæˆ–åˆ†å—ï¼‰ä¸­ç§»é™¤æ‰€æœ‰ç‰¹æ®Š tokenã€‚å¿…é¡»ç¡®ä¿åœ¨ç‰¹æ®Š token å¤„åˆ†å‰²æ–‡æœ¬ï¼Œé˜²æ­¢è·¨æ–‡æ¡£åˆå¹¶ã€‚ä¾‹å¦‚ï¼Œå¯¹äº `[æ–‡æ¡£1] [æ–‡æ¡£2]` çš„è¯­æ–™ï¼Œåº”åœ¨ å¤„åˆ†å‰²ï¼Œåˆ†åˆ«å¯¹ä¸¤éƒ¨åˆ†è¿›è¡Œé¢„åˆ†è¯ã€‚å¯é€šè¿‡ `re.split` å®ç°ï¼Œä½¿ç”¨ `\"|\".join(special_tokens)` ä½œä¸ºåˆ†éš”ç¬¦ï¼ˆæ³¨æ„ä½¿ç”¨ `re.escape` å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å… `|` ç­‰ç¬¦å·å¼•å‘é—®é¢˜ï¼‰ã€‚æµ‹è¯•ç”¨ä¾‹ `test_train_byte_special_tokens` å°†éªŒè¯è¯¥è¡Œä¸ºã€‚  \n\n### ä¼˜åŒ–åˆå¹¶æ­¥éª¤  \n\næœ´ç´ çš„ BPE è®­ç»ƒå®ç°æ•ˆç‡è¾ƒä½ï¼Œå› æ¯æ¬¡åˆå¹¶åéœ€é‡æ–°éå†æ‰€æœ‰å­—èŠ‚å¯¹ä»¥æ‰¾æœ€é«˜é¢‘å¯¹ã€‚å®é™…ä¸Šï¼Œä»…ä¸è¢«åˆå¹¶å­—èŠ‚å¯¹ç›¸é‚»çš„è®¡æ•°ä¼šå‘ç”Ÿå˜åŒ–ã€‚å› æ­¤ï¼Œå¯é€šè¿‡ç»´æŠ¤å­—èŠ‚å¯¹è®¡æ•°çš„ç´¢å¼•ç»“æ„ï¼Œå¹¶åœ¨æ¯æ¬¡åˆå¹¶åå¢é‡æ›´æ–°ç›¸å…³è®¡æ•°ï¼Œæ˜¾è‘—æå‡é€Ÿåº¦ã€‚å°½ç®¡è¯¥ç¼“å­˜æœºåˆ¶èƒ½å¤§å¹…åŠ é€Ÿï¼Œä½† BPE åˆå¹¶æ­¥éª¤åœ¨ Python ä¸­æ— æ³•å¹¶è¡ŒåŒ–ã€‚  \n\n### ä½èµ„æº/é™çº§æç¤ºï¼šæ€§èƒ½åˆ†æ  \n\nå»ºè®®ä½¿ç”¨ `cProfile` æˆ– `scalene` ç­‰å·¥å…·è¿›è¡Œæ€§èƒ½åˆ†æï¼Œå®šä½ç“¶é¢ˆå¹¶é›†ä¸­ä¼˜åŒ–å…³é”®éƒ¨åˆ†ã€‚  \n\n### ä½èµ„æº/é™çº§æç¤ºï¼šé™çº§è®­ç»ƒ  \n\nä¸è¦ç›´æ¥åœ¨å®Œæ•´ TinyStories æ•°æ®é›†ä¸Šè®­ç»ƒã€‚å»ºè®®å…ˆåœ¨å°è§„æ¨¡å­é›†ï¼ˆâ€œè°ƒè¯•æ•°æ®é›†â€ï¼‰ä¸Šå®éªŒï¼Œä¾‹å¦‚ä½¿ç”¨åŒ…å« 2.2 ä¸‡ç¯‡æ–‡æ¡£çš„éªŒè¯é›†ï¼ˆè€Œéå®Œæ•´çš„ 212 ä¸‡ç¯‡ï¼‰ã€‚è¿™æ˜¯ä¸€ç§é€šç”¨å¼€å‘ç­–ç•¥ï¼šé€šè¿‡ä½¿ç”¨æ›´å°æ•°æ®ã€æ›´å°æ¨¡å‹æ¥åŠ é€Ÿè¿­ä»£ã€‚é€‰æ‹©è°ƒè¯•é›†è§„æ¨¡å’Œè¶…å‚æ•°æ—¶éœ€æƒè¡¡ï¼šåº”è¶³å¤Ÿå¤§ä»¥å¤ç°çœŸå®ç“¶é¢ˆï¼Œåˆä¸å®œè¿‡å¤§å¯¼è‡´è€—æ—¶è¿‡é•¿ã€‚"},{"cell_type":"markdown","metadata":{"id":"0F5DCA157E614507962403EF1B568440","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### é—®é¢˜ï¼ˆ15åˆ†ï¼‰BPE åˆ†è¯å™¨è®­ç»ƒ  \n\näº¤ä»˜è¦æ±‚ï¼šç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ ¹æ®è¾“å…¥çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„è®­ç»ƒï¼ˆå­—èŠ‚çº§ï¼‰BPEåˆ†è¯å™¨ã€‚ä½ çš„BPEè®­ç»ƒå‡½æ•°åº”è‡³å°‘å¤„ç†ä»¥ä¸‹è¾“å…¥å‚æ•°ï¼š  \n\ninput_path: str BPEåˆ†è¯å™¨è®­ç»ƒæ•°æ®æ–‡æœ¬æ–‡ä»¶çš„è·¯å¾„ã€‚  \n\nvocab_size: int å®šä¹‰æœ€ç»ˆè¯æ±‡è¡¨æœ€å¤§å¤§å°çš„æ­£æ•´æ•°ï¼ˆåŒ…æ‹¬åˆå§‹å­—èŠ‚è¯æ±‡è¡¨ã€åˆå¹¶äº§ç”Ÿçš„è¯æ±‡è¡¨é¡¹å’Œä»»ä½•ç‰¹æ®Štokenï¼‰ã€‚  \n\nspecial_tokens: list[str] éœ€è¦æ·»åŠ åˆ°è¯æ±‡è¡¨ä¸­çš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚è¿™äº›ç‰¹æ®Štokenä¸ä¼šå½±å“BPEè®­ç»ƒè¿‡ç¨‹ã€‚  \n\nä½ çš„BPEè®­ç»ƒå‡½æ•°åº”è¿”å›æœ€ç»ˆçš„è¯æ±‡è¡¨å’Œåˆå¹¶è®°å½•ï¼š  \n\nvocab: dict[int, bytes] åˆ†è¯å™¨è¯æ±‡è¡¨ï¼Œä¸€ä¸ªä»æ•´æ•°ï¼ˆè¯æ±‡è¡¨ä¸­çš„token IDï¼‰åˆ°å­—èŠ‚ï¼ˆtokenå­—èŠ‚ï¼‰çš„æ˜ å°„ã€‚  \n\nmerges: list[tuple[bytes, bytes]] è®­ç»ƒäº§ç”Ÿçš„BPEåˆå¹¶è®°å½•åˆ—è¡¨ã€‚æ¯ä¸ªåˆ—è¡¨é¡¹æ˜¯ä¸€ä¸ªå­—èŠ‚å…ƒç»„(<token1>, <token2>)ï¼Œè¡¨ç¤º<token1>ä¸<token2>è¢«åˆå¹¶ã€‚  \n\tå‚è€ƒï¼šhttps://github.com/weiruihhh/cs336_note_and_hw"},{"cell_type":"markdown","metadata":{"id":"D3C85A85702C49088E04FBD49D6E8104","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### ğŸ” **ç®€å•æ¡ˆä¾‹ï¼šBPEå­—èŠ‚å¯¹åˆå¹¶ä»£ç å®ç°æ¼”ç¤º**  \n\n#### **æˆ‘ä»¬è¦åšä»€ä¹ˆï¼Ÿ**  \næˆ‘ä»¬å°†æ¼”ç¤ºå¦‚ä½•å°†å­—èŠ‚å¯¹ `('a','b')` åˆå¹¶ä¸ºæ–° token `'ab'`ï¼Œ  \né‡ç‚¹å…³æ³¨**ç´¢å¼•æ›´æ–°æœºåˆ¶**å’Œ**é“¾å¼å½±å“å¤„ç†**ï¼Œ  \nå±•ç¤º BPE ç®—æ³•å¦‚ä½•**ç²¾å‡†ç»´æŠ¤å­—èŠ‚å¯¹é¢‘ç‡**ã€‚  \n\n---  \n\n### ğŸŒ° **æ¡ˆä¾‹è®¾å®š**  \n**è¯­æ–™åº“**ï¼ˆä»…1ä¸ªåºåˆ—ï¼‰ï¼š  \n```python  \nself.sequences = [  \n    ['a', 'b', 'c', 'a', 'b', 'd']  # åºåˆ—0  \n]  \n```\n\n**åˆå§‹ç´¢å¼•çŠ¶æ€**ï¼š  \n```python  \nself.pair_positions[('a','b')] = [(0,0), (0,3)]  # åœ¨åºåˆ—0çš„ä½ç½®0å’Œ3å‡ºç°  \nself.pair_counts = {  \n    ('a','b'): 2,  \n    ('b','c'): 1,  \n    ('c','a'): 1,  \n    ('b','d'): 1  \n}  \n```\n\n**ç›®æ ‡**ï¼š  \nåˆå¹¶ `('a','b') â†’ 'ab'`ï¼Œ  \nå¹¶**è‡ªåŠ¨æ›´æ–°æ‰€æœ‰å—å½±å“çš„å­—èŠ‚å¯¹**ã€‚  \n\n---  \n\n### ğŸš€ **æ‰§è¡Œæµç¨‹è¯¦è§£**  \n\n#### **æ­¥éª¤1ï¼šæŒ‰åºåˆ—åˆ†ç»„ä½ç½®**  \n```python  \npositions_by_seq = defaultdict(list)  \nfor seq_idx, pos in self.pair_positions[pair]:  \n    positions_by_seq[seq_idx].append(pos)  \n```\n- **è¾“å…¥**ï¼š`self.pair_positions[('a','b')] = [(0,0), (0,3)]`  \n- **è¾“å‡º**ï¼š`positions_by_seq = {0: [0, 3]}`  \n- **ä¸ºä»€ä¹ˆ**ï¼šå°†å…¨å±€ä½ç½®æŒ‰**åºåˆ—IDåˆ†ç»„**ï¼Œå‡†å¤‡ç‹¬ç«‹å¤„ç†  \n\n---  \n\n#### **æ­¥éª¤2ï¼šå€’åºå¤„ç†åºåˆ—0**  \n```python  \nseq = ['a', 'b', 'c', 'a', 'b', 'd']  # åºåˆ—0  \npositions = [0, 3] â†’ å€’åºå [3, 0]    # å…³é”®ï¼šä»åå¾€å‰å¤„ç†  \nlast_merged_pos = -2                  # åˆå§‹åŒ–æ ‡è®°  \n```\n\n---  \n\n#### **æ­¥éª¤3ï¼šå¤„ç†ä½ç½®3ï¼ˆé¦–æ¬¡åˆå¹¶ï¼‰**  \n```python  \npos = 3  \n# æ£€æŸ¥æœ‰æ•ˆæ€§ï¼ˆé€šè¿‡ï¼‰  \n# å†…å®¹éªŒè¯ï¼šseq[3]='a', seq[4]='b' â†’ åŒ¹é… ('a','b')  \n```\n\n**æ‰§è¡Œåˆå¹¶**ï¼š  \n```python  \nseq[3] = 'ab'   # ['a','b','c','ab','b','d']  \ndel seq[4]      # ['a','b','c','ab','d']  \nmerge_count = 1  \nlast_merged_pos = 3  \n```\n\n**æ›´æ–°å·¦ä¾§å­—èŠ‚å¯¹**ï¼ˆ`c a â†’ c ab`ï¼‰ï¼š  \n```python  \nleft_pair = ('c', 'a') â†’ è®¡æ•°-1ï¼ˆä»1â†’0ï¼‰  \nnew_left_pair = ('c', 'ab') â†’ è®¡æ•°+1ï¼ˆä»0â†’1ï¼‰  \nè®°å½•æ–°ä½ç½®ï¼šself.pair_positions[('c','ab')] = [(0,2)]  \n```\n\n**æ›´æ–°å³ä¾§å­—èŠ‚å¯¹**ï¼ˆ`b d â†’ ab d`ï¼‰ï¼š  \n```python  \nright_pair = ('b', 'd') â†’ è®¡æ•°-1ï¼ˆä»1â†’0ï¼‰  \nnew_right_pair = ('ab', 'd') â†’ è®¡æ•°+1ï¼ˆä»0â†’1ï¼‰  \nè®°å½•æ–°ä½ç½®ï¼šself.pair_positions[('ab','d')] = [(0,3)]  \n```\n\n**å½“å‰çŠ¶æ€**ï¼š  \n```python  \nsequences = [['a','b','c','ab','d']]  \npair_counts = {  \n    ('a','b'): 1,   # ä»2â†’1ï¼ˆä½ç½®0ä»å­˜åœ¨ï¼‰  \n    ('b','c'): 1,  \n    ('ab','d'): 1,  # æ–°å¢  \n    ('c','ab'): 1   # æ–°å¢  \n}  \n```\n\n---  \n\n#### **æ­¥éª¤4ï¼šå¤„ç†ä½ç½®0ï¼ˆäºŒæ¬¡åˆå¹¶ï¼‰**  \n```python  \npos = 0  \n# æ£€æŸ¥æœ‰æ•ˆæ€§ï¼ˆé€šè¿‡ï¼‰  \n# å†…å®¹éªŒè¯ï¼šseq[0]='a', seq[1]='b' â†’ ä»åŒ¹é… ('a','b')  \n```\n\n**æ‰§è¡Œåˆå¹¶**ï¼š  \n```python  \nseq[0] = 'ab'   # ['ab','b','c','ab','d']  \ndel seq[1]      # ['ab','c','ab','d']  \nmerge_count = 2  \nlast_merged_pos = 0  \n```\n\n**æ›´æ–°å·¦ä¾§å­—èŠ‚å¯¹**ï¼šæ— ï¼ˆä½ç½®0æ— å·¦ä¾§ï¼‰  \n\n**æ›´æ–°å³ä¾§å­—èŠ‚å¯¹**ï¼ˆ`b c â†’ ab c`ï¼‰ï¼š  \n```python  \nright_pair = ('b', 'c') â†’ è®¡æ•°-1ï¼ˆä»1â†’0ï¼‰  \nnew_right_pair = ('ab', 'c') â†’ è®¡æ•°+1ï¼ˆä»0â†’1ï¼‰  \nè®°å½•æ–°ä½ç½®ï¼šself.pair_positions[('ab','c')] = [(0,0)]  \n```\n\n**æœ€ç»ˆçŠ¶æ€**ï¼š  \n```python  \nsequences = [['ab','c','ab','d']]  \n\npair_counts = {  \n    ('ab','c'): 1,  # æ–°å¢  \n    ('c','ab'): 1,  # ä¿ç•™  \n    ('ab','d'): 1,  # ä¿ç•™  \n    ('a','b'): 0    # ä»1â†’0ï¼ˆæ¸…ç†ï¼‰  \n}  \n\npair_positions = {  \n    ('ab','c'): [(0,0)],  \n    ('c','ab'): [(0,1)],  \n    ('ab','d'): [(0,2)]  \n}  \n```\n\n"},{"cell_type":"markdown","metadata":{"id":"20F41A9F791B4457976ED24200E0FBFE","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### é—®é¢˜ ï¼ˆtrain_bpe_tinystoriesï¼‰ï¼šTinyStories çš„ BPE åŸ¹è®­ï¼ˆ2 åˆ†ï¼‰  \n(1) åœ¨ TinyStories æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªå­—èŠ‚çº§ BPE åˆ†è¯å™¨ï¼Œæœ€å¤§è¯æ±‡è¡¨å¤§å°ä¸º 10,000ã€‚ç¡®ä¿å°† TinyStories çš„ `</s>` ç‰¹æ®Šæ ‡è®°åŠ å…¥è¯æ±‡è¡¨ã€‚å°†ç”Ÿæˆçš„è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™åºåˆ—åŒ–ä¿å­˜åˆ°ç£ç›˜ä»¥ä¾¿è¿›ä¸€æ­¥æ£€æŸ¥ã€‚è®­ç»ƒè€—æ—¶å¤šå°‘å°æ—¶ï¼Œå ç”¨å¤šå°‘å†…å­˜ï¼Ÿè¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token æ˜¯ä»€ä¹ˆï¼Ÿè¿™åˆç†å—ï¼Ÿ  \nèµ„æºè¦æ±‚ï¼šâ‰¤ 30 åˆ†é’Ÿï¼ˆä¸ä½¿ç”¨ GPUï¼‰ï¼Œâ‰¤ 30GB å†…å­˜  \næç¤ºï¼šé€šè¿‡åœ¨é¢„åˆ†è¯é˜¶æ®µä½¿ç”¨å¤šè¿›ç¨‹ï¼Œå¹¶ç»“åˆä»¥ä¸‹ä¸¤ä¸ªäº‹å®ï¼Œä½ åº”è¯¥èƒ½åœ¨ 2 åˆ†é’Ÿå†…å®Œæˆ BPE è®­ç»ƒï¼š  \n(a) æ•°æ®æ–‡ä»¶ä¸­ä½¿ç”¨ `</s>` æ ‡è®°æ¥åˆ†éš”æ–‡æ¡£ã€‚  \n(b) `</s>` æ ‡è®°åœ¨åº”ç”¨ BPE åˆå¹¶ä¹‹å‰å·²è¢«ä½œä¸ºç‰¹æ®Šæƒ…å†µå¤„ç†ã€‚  \näº¤ä»˜å†…å®¹ï¼šä¸€åˆ°ä¸¤å¥è¯çš„å›å¤ã€‚  \n(2) å¯¹ä½ çš„ä»£ç è¿›è¡Œæ€§èƒ½åˆ†æã€‚åˆ†è¯å™¨è®­ç»ƒè¿‡ç¨‹ä¸­å“ªä¸€éƒ¨åˆ†è€—æ—¶æœ€é•¿ï¼Ÿ  \näº¤ä»˜å†…å®¹ï¼šä¸€åˆ°ä¸¤å¥è¯çš„å›å¤ã€‚  \n"},{"cell_type":"code","metadata":{"id":"810CF482643D4985AFA558B9F60AF78D","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true,"collapsed":false,"scrolled":false},"source":"import os\nimport heapq\nimport regex\nimport time\nimport random\nimport multiprocessing\nfrom functools import partial\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict, DefaultDict, Any, Union\nimport mmap\nimport re\nfrom collections import defaultdict\n\n# GPT-2é¢„åˆ†è¯æ¨¡å¼\nGPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n\ndef load_and_sample_data(file_path: str, sample_size: int = 22000, special_token: str = \"<|endoftext|>\") -> str:\n    \"\"\"å†…å­˜æ˜ å°„æ–¹å¼åŠ è½½å¹¶é‡‡æ ·æ–‡æ¡£\"\"\"\n    try:\n        with open(file_path, \"r+\", encoding='utf-8', errors='ignore') as f:\n            with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n                documents = []\n                start = 0\n                while start < len(mm):\n                    end = mm.find(special_token.encode('utf-8'), start)\n                    if end == -1:\n                        doc = mm[start:].decode('utf-8', errors='replace').strip()\n                        if doc:\n                            documents.append(doc)\n                        break\n                    \n                    doc = mm[start:end].decode('utf-8', errors='replace').strip()\n                    if doc:\n                        documents.append(doc)\n                    start = end + len(special_token)\n                \n                if len(documents) > sample_size:\n                    documents = random.sample(documents, sample_size)\n                \n                return special_token.join(documents)\n    except Exception as e:\n        raise IOError(f\"åŠ è½½æ•°æ®é›†å¤±è´¥: {e}\")\n\ndef gpt2_bytes_to_unicode_local() -> Dict[int, str]:\n    \"\"\"å­—èŠ‚åˆ°Unicodeæ˜ å°„\"\"\"\n    bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256)) \n    cs = bs[:]\n    n = 0\n    for b in range(256):\n        if b not in bs:\n            bs.append(b)\n            cs.append(256 + n)\n            n += 1\n    return {b: chr(c) for b, c in zip(bs, cs)}\n\ndef pre_tokenize_document(doc: str, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:\n    \"\"\"é¢„åˆ†è¯å¤„ç†å•ä¸ªæ–‡æ¡£\"\"\"\n    tokens = regex.findall(GPT2_SPLIT_PATTERN, doc, flags=regex.UNICODE)\n    sequences = []\n    for token in tokens:\n        token_unicode = ''.join(bytes_to_unicode_map[b] for b in token.encode('utf-8'))\n        sequences.append(list(token_unicode))\n    return sequences\n\ndef parallel_pre_tokenize(documents: List[str], num_processes: int, bytes_to_unicode_map: Dict[int, str]) -> List[List[str]]:\n    \"\"\"å¹¶è¡Œé¢„åˆ†è¯ä¼˜åŒ–\"\"\"\n    if num_processes <= 1:\n        return [seq for doc in documents for seq in pre_tokenize_document(doc, bytes_to_unicode_map)]\n    \n    with multiprocessing.Pool(\n        num_processes,\n        initializer=init_worker,\n        initargs=(bytes_to_unicode_map,)\n    ) as pool:\n        results = list(tqdm(\n            pool.imap(pre_tokenize_worker, documents, chunksize=50),\n            total=len(documents),\n            desc=\"é¢„åˆ†è¯\",\n            mininterval=1\n        ))\n    return [seq for doc_sequences in results for seq in doc_sequences]\n\n# å…¨å±€å˜é‡ç”¨äºå¤šè¿›ç¨‹\nglobal_worker_byte_map = None\ndef init_worker(byte_map: Dict[int, str]):\n    global global_worker_byte_map\n    global_worker_byte_map = byte_map\n\ndef pre_tokenize_worker(doc: str) -> List[List[str]]:\n    return pre_tokenize_document(doc, global_worker_byte_map)\n\nclass BPEIndex:\n    \"\"\"é«˜æ•ˆç´¢å¼•ç»“æ„ç”¨äºBPEåˆå¹¶\"\"\"\n    def __init__(self, sequences: List[List[str]]):\n        self.sequences = sequences # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬åºåˆ—\n        self.pair_counts: DefaultDict[Tuple[str, str], int] = defaultdict(int) # ç»Ÿè®¡å­—èŠ‚å¯¹é¢‘ç‡\n        self.pair_positions: DefaultDict[Tuple[str, str], List[Tuple[int, int]]] = defaultdict(list) # è®°å½•å­—èŠ‚å¯¹ä½ç½®\n        self.heap = []  # æœ€å¤§å †ï¼ˆå­˜æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼‰\n        self.heap_entries: Dict[Tuple[str, str], Any] = {} # å †æ¡ç›®å¿«é€Ÿè®¿é—®\n         \n        # åˆå§‹åŒ–ç´¢å¼• ä¸€æ¬¡æ€§ç»Ÿè®¡æ‰€æœ‰ç›¸é‚»å­—èŠ‚å¯¹çš„å‡ºç°ä½ç½®å’Œé¢‘ç‡â€”â€”å°†ä¸å¯è¡Œçš„O(NÂ²)é—®é¢˜è½¬åŒ–ä¸ºå¯å¤„ç†çš„O(N log N)\n        for seq_idx, seq in enumerate(sequences):\n            for pos in range(len(seq) - 1):\n                pair = (seq[pos], seq[pos + 1])\n                self.pair_counts[pair] += 1\n                self.pair_positions[pair].append((seq_idx, pos))\n        \n        # æ„å»ºå † å°†é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆ>1æ¬¡ï¼‰åŠ å…¥æœ€å¤§å †ï¼Œè®© get_most_frequent() èƒ½ O(1) è·å–æœ€é«˜é¢‘å¯¹ã€‚\n        for pair, count in self.pair_counts.items():\n            if count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair\n                entry = [-count, pair]\n                heapq.heappush(self.heap, entry)\n                self.heap_entries[pair] = entry\n    \n    def get_most_frequent(self) -> Tuple[str, str]:\n        \"\"\"å¿«é€Ÿè¿”å›å½“å‰æœ€é«˜é¢‘å­—èŠ‚å¯¹ï¼ˆè·³è¿‡å·²è¢«åˆå¹¶çš„æ— æ•ˆæ¡ç›®ï¼‰\"\"\"\n        while self.heap:\n            neg_count, pair = self.heap[0]\n            # æ£€æŸ¥pairæ˜¯å¦ä»ç„¶æœ‰æ•ˆ\n            if pair not in self.heap_entries:\n                heapq.heappop(self.heap)\n                continue\n                \n            current_count = self.pair_counts.get(pair, 0)\n            \n            # æ£€æŸ¥è®¡æ•°æ˜¯å¦åŒ¹é…ä¸”å¤§äº1\n            if -neg_count == current_count and current_count > 1:\n                return pair\n            # å¦åˆ™ç§»é™¤æ— æ•ˆæ¡ç›®\n            heapq.heappop(self.heap)\n            if pair in self.heap_entries:  # ç¡®ä¿æ¡ç›®å­˜åœ¨\n                del self.heap_entries[pair]\n        return None\n    \n    def merge_pair(self, pair: Tuple[str, str], new_token: str) -> int:\n        \"\"\"åˆå¹¶å­—ç¬¦å¯¹å¹¶æ›´æ–°ç´¢å¼•\"\"\"\n        if pair not in self.pair_positions or not self.pair_positions[pair]:\n            return 0\n        \n        # æŒ‰åºåˆ—å’Œä½ç½®åˆ†ç»„\n        positions_by_seq = defaultdict(list)\n        for seq_idx, pos in self.pair_positions[pair]:\n            positions_by_seq[seq_idx].append(pos)\n        \n        merge_count = 0\n        for seq_idx, positions in positions_by_seq.items():\n            seq = self.sequences[seq_idx]\n            # æŒ‰ä½ç½®å€’åºæ’åº\n            positions.sort(reverse=True)\n            last_merged_pos = -2\n            \n            for pos in positions:\n                # æ£€æŸ¥æ˜¯å¦å·²è¢«å‰é¢çš„åˆå¹¶å½±å“\n                if pos >= len(seq) - 1 or pos <= last_merged_pos:\n                    continue\n                if seq[pos] != pair[0] or seq[pos + 1] != pair[1]:\n                    continue\n                \n                # æ‰§è¡Œåˆå¹¶\n                seq[pos] = new_token\n                del seq[pos + 1]\n                merge_count += 1\n                last_merged_pos = pos\n                \n                # æ›´æ–°å·¦ä¾§pair\n                if pos > 0:\n                    left_pair = (seq[pos - 1], pair[0])\n                    self._update_pair_count(left_pair, -1)\n                    \n                    new_left_pair = (seq[pos - 1], new_token)\n                    self._update_pair_count(new_left_pair, 1)\n                    self._add_position(new_left_pair, seq_idx, pos - 1)\n                \n                # æ›´æ–°å³ä¾§pair\n                if pos < len(seq) - 1:\n                    right_pair = (pair[1], seq[pos + 1])\n                    self._update_pair_count(right_pair, -1)\n                    \n                    new_right_pair = (new_token, seq[pos + 1])\n                    self._update_pair_count(new_right_pair, 1)\n                    self._add_position(new_right_pair, seq_idx, pos)\n        \n        # æ¸…ç†å·²åˆå¹¶çš„pair\n        if pair in self.pair_counts:\n            del self.pair_counts[pair]\n        if pair in self.pair_positions:\n            del self.pair_positions[pair]\n        if pair in self.heap_entries:\n            # æ ‡è®°ä¸ºæ— æ•ˆï¼Œç¨åæ¸…ç†\n            self.heap_entries[pair] = None\n        \n        return merge_count\n    \n    def _update_pair_count(self, pair: Tuple[str, str], delta: int):\n        \"\"\"æ›´æ–°å­—ç¬¦å¯¹è®¡æ•°\"\"\"\n        if delta == 0:\n            return\n        \n        # ç¡®ä¿pairå­˜åœ¨äºå­—å…¸ä¸­\n        if pair not in self.pair_counts:\n            self.pair_counts[pair] = 0\n            \n        new_count = self.pair_counts[pair] + delta\n        self.pair_counts[pair] = new_count\n        \n        # ç¡®ä¿è®¡æ•°ä¸ä¸ºè´Ÿ\n        if new_count < 0:\n            new_count = 0\n            self.pair_counts[pair] = 0\n        \n        if pair in self.heap_entries and self.heap_entries[pair] is not None:\n            # æ›´æ–°å †æ¡ç›®\n            self.heap_entries[pair][0] = -new_count\n            heapq.heapify(self.heap)\n        elif new_count > 1:  # åªæ·»åŠ è®¡æ•°å¤§äº1çš„pair\n            # æ–°å»ºå †æ¡ç›®\n            entry = [-new_count, pair]\n            heapq.heappush(self.heap, entry)\n            self.heap_entries[pair] = entry\n    \n    def _add_position(self, pair: Tuple[str, str], seq_idx: int, pos: int):\n        \"\"\"æ·»åŠ æ–°ä½ç½®åˆ°ç´¢å¼•\"\"\"\n        self.pair_positions[pair].append((seq_idx, pos))\n\ndef run_train_bpe(\n    input_path: Union[str, os.PathLike],\n    vocab_size: int,\n    special_tokens: List[str] = [\"<|endoftext|>\"],\n    num_processes: int = 8,\n    sample_size: int = 22000,\n    **kwargs,\n) -> Tuple[Dict[int, bytes], List[Tuple[bytes, bytes]]]:\n    # å‚æ•°éªŒè¯\n    base_vocab_size = 256 + len(special_tokens)\n    if vocab_size < base_vocab_size:\n        raise ValueError(f\"vocab_sizeè‡³å°‘éœ€{base_vocab_size}\")\n    \n    # 1. å­—èŠ‚åˆ°Unicodeæ˜ å°„\n    bytes_to_unicode_map = gpt2_bytes_to_unicode_local()\n    unicode_to_bytes_map = {v: bytes([k]) for k, v in bytes_to_unicode_map.items()}\n    \n    # 2. åˆå§‹åŒ–è¯æ±‡è¡¨\n    vocab = {i: bytes([i]) for i in range(256)}\n    next_token_id = 256\n    existing_bytes = set(vocab.values())\n    \n    # 3. æ·»åŠ ç‰¹æ®Štoken\n    for st in special_tokens:\n        st_bytes = st.encode(\"utf-8\")\n        if st_bytes not in existing_bytes and len(vocab) < vocab_size:\n            vocab[next_token_id] = st_bytes\n            existing_bytes.add(st_bytes)\n            next_token_id += 1\n    \n    # 4. åŠ è½½å¹¶é‡‡æ ·æ•°æ®\n    print(f\"ğŸ“– ä» {input_path} åŠ è½½å¹¶é‡‡æ · {sample_size} ä¸ªæ–‡æ¡£...\")\n    text = load_and_sample_data(input_path, sample_size, special_tokens[0])\n    \n    # 5. åˆ†å‰²æ–‡æ¡£\n    escaped_tokens = [re.escape(st) for st in special_tokens]   ## è¿”å› \"<\\|endoftext\\|>\"\n    split_pattern = \"|\".join(escaped_tokens) \n    documents = [part for part in re.split(split_pattern, text) if part]\n    \n    # 6. å¹¶è¡Œé¢„åˆ†è¯\n    sequences = parallel_pre_tokenize(documents, num_processes, bytes_to_unicode_map)\n    print(f\"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° {len(sequences):,} ä¸ªtokenåºåˆ—\")\n    \n    # 7. åˆå§‹åŒ–ç´¢å¼•ç»“æ„\n    print(\"ğŸ”§ æ„å»ºBPEç´¢å¼•...\")\n    bpe_index = BPEIndex(sequences)\n    merges = []\n    vocab_progress = len(vocab)\n    total_merges = vocab_size - vocab_progress\n    \n    # 8. BPEè®­ç»ƒä¸»å¾ªç¯\n    print(f\"ğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡åˆå¹¶æ•°: {total_merges:,}\")\n    progress_bar = tqdm(total=total_merges, desc=\"è®­ç»ƒBPE\", unit=\"åˆå¹¶\", mininterval=0.5)\n    \n    while vocab_progress < vocab_size:\n        best_pair = bpe_index.get_most_frequent()\n        if best_pair is None:\n            print(\"\\nâš ï¸ æ²¡æœ‰æ›´å¤šæœ‰æ•ˆçš„å­—ç¬¦å¯¹å¯ä¾›åˆå¹¶ï¼Œæå‰ç»“æŸè®­ç»ƒ\")\n            break\n        \n        # åˆ›å»ºæ–°token\n        new_token_str = best_pair[0] + best_pair[1]\n        p1_bytes = unicode_to_bytes_map[best_pair[0]]\n        p2_bytes = unicode_to_bytes_map[best_pair[1]]\n        new_token_bytes = p1_bytes + p2_bytes\n        \n        # æ‰§è¡Œåˆå¹¶\n        merge_count = bpe_index.merge_pair(best_pair, new_token_str)\n        if merge_count == 0:\n            continue\n        \n        # æ›´æ–°è¯æ±‡è¡¨\n        if new_token_bytes not in existing_bytes:\n            vocab[next_token_id] = new_token_bytes\n            existing_bytes.add(new_token_bytes)\n            merges.append((p1_bytes, p2_bytes))\n            next_token_id += 1\n            vocab_progress += 1\n            progress_bar.update(1)\n        \n        # æ›´æ–°æ˜ å°„è¡¨\n        unicode_to_bytes_map[new_token_str] = new_token_bytes\n    \n    progress_bar.close()\n    return vocab, merges\n\ndef evaluate_tokenizer(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], test_text: str):\n    \"\"\"ç®€å•è¯„ä¼°åˆ†è¯å™¨æ•ˆæœ\"\"\"\n    print(\"\\nğŸ” åˆ†è¯å™¨è¯„ä¼°\")\n    sample_text = test_text[:200] + \"...\" if len(test_text) > 200 else test_text\n    print(f\"æ ·ä¾‹æ–‡æœ¬: {sample_text}\")\n    \n    # ç®€å•ç»Ÿè®¡\n    unique_tokens = set(vocab.values())\n    print(f\"è¯æ±‡è¡¨å¤§å°: {len(vocab):,}\")\n    print(f\"å”¯ä¸€tokenæ•°: {len(unique_tokens):,}\")\n    print(f\"åˆå¹¶æ“ä½œæ•°: {len(merges):,}\")\n\nif __name__ == \"__main__\":\n    # é…ç½®å‚æ•°\n    config = {\n        \"vocab_size\": 10000,\n        \"special_tokens\": [\"<|endoftext|>\", \"<pad>\", \"<unk>\"],\n        \"num_processes\": 8,\n        \"sample_size\": 22000,  # åˆå§‹é‡‡æ ·22,000æ–‡æ¡£\n    }\n    \n    # æ•°æ®é›†è·¯å¾„\n    train_path = \"/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt\"\n    valid_path = \"/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-valid.txt\"\n    \n    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n    if not Path(train_path).exists():\n        raise FileNotFoundError(f\"è®­ç»ƒé›†æ–‡ä»¶ {train_path} ä¸å­˜åœ¨\")\n    if not Path(valid_path).exists():\n        raise FileNotFoundError(f\"éªŒè¯é›†æ–‡ä»¶ {valid_path} ä¸å­˜åœ¨\")\n    \n    # è®­ç»ƒæ¨¡å‹\n    print(\"ğŸš€ å¼€å§‹è®­ç»ƒ\")\n    start_time = time.time()\n    \n    train_vocab, train_merges = run_train_bpe(train_path, **config)\n    \n    print(f\"\\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: {time.time() - start_time:.2f}ç§’\")\n    \n    # å°è§„æ¨¡éªŒè¯ (ä½¿ç”¨éªŒè¯é›†çš„10%)\n    print(\"\\nğŸ”¬ å°è§„æ¨¡éªŒè¯\")\n    valid_config = config.copy()\n    valid_config[\"sample_size\"] = int(2)  # éªŒè¯é›†ä½¿ç”¨500æ–‡æ¡£ (10%)\n    \n    valid_vocab, valid_merges = run_train_bpe(valid_path, **valid_config)\n    \n    # åˆ†æç»“æœ\n    print(\"\\nğŸ“Š è®­ç»ƒç»“æœ\")\n    print(f\"è®­ç»ƒè¯æ±‡è¡¨å¤§å°: {len(train_vocab):,}\")\n    print(f\"è®­ç»ƒåˆå¹¶æ“ä½œæ•°: {len(train_merges):,}\")\n    print(f\"éªŒè¯è¯æ±‡è¡¨å¤§å°: {len(valid_vocab):,}\")\n    print(f\"éªŒè¯åˆå¹¶æ“ä½œæ•°: {len(valid_merges):,}\")\n    \n    # æ¯”è¾ƒè¯æ±‡è¡¨é‡å ç‡\n    train_tokens = set(train_vocab.values())\n    valid_tokens = set(valid_vocab.values())\n    overlap = train_tokens & valid_tokens\n    print(f\"\\nğŸ“ˆ è¯æ±‡è¡¨é‡å ç‡: {len(overlap)/len(train_tokens):.1%}\")\n    \n    # åŠ è½½éªŒè¯é›†æ ·ä¾‹è¿›è¡Œè¯„ä¼°\n    with open(valid_path, \"r\", encoding=\"utf-8\") as f:\n        valid_text = f.read(1000)  # è¯»å–å‰1000å­—ç¬¦ç”¨äºè¯„ä¼°\n    evaluate_tokenizer(train_vocab, train_merges, valid_text)\n\n    import json  # éœ€è¦å¯¼å…¥jsonæ¨¡å—\n\n    # åœ¨mainå‡½æ•°æœ«å°¾æ·»åŠ ä»¥ä¸‹ä»£ç ï¼ˆåœ¨å†…å­˜åˆ†æä¹‹å‰ï¼‰\n    def save_vocab_and_merges(vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]], vocab_path: str, merges_path: str):\n        \"\"\"ä¿å­˜è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨åˆ°æ–‡ä»¶\"\"\"\n        # 1. ä¿å­˜è¯æ±‡è¡¨ (JSONæ ¼å¼)\n        vocab_str = {idx: token.decode('utf-8', errors='replace') for idx, token in vocab.items()}\n        with open(vocab_path, 'w', encoding='utf-8') as f:\n            json.dump(vocab_str, f, ensure_ascii=False, indent=2)\n        \n        # 2. ä¿å­˜åˆå¹¶åˆ—è¡¨ (æ–‡æœ¬æ ¼å¼)\n        with open(merges_path, 'w', encoding='utf-8') as f:\n            for merge in merges:\n                part1 = merge[0].decode('utf-8', errors='replace')\n                part2 = merge[1].decode('utf-8', errors='replace')\n                f.write(f\"{part1} {part2}\\n\")\n\n    # åœ¨mainå‡½æ•°ä¸­è°ƒç”¨ä¿å­˜åŠŸèƒ½ï¼ˆåœ¨è®­ç»ƒå®Œæˆåï¼‰\n    output_dir = \"/home/mw/project\"  # ä¿®æ”¹ä¸ºæ‚¨çš„è¾“å‡ºç›®å½•\n    os.makedirs(output_dir, exist_ok=True)\n\n    vocab_path = os.path.join(output_dir, \"gpt2_vocab.json\")\n    merges_path = os.path.join(output_dir, \"gpt2_merges.txt\")\n\n    save_vocab_and_merges(train_vocab, train_merges, vocab_path, merges_path)\n    print(f\"âœ… è¯æ±‡è¡¨å·²ä¿å­˜è‡³: {vocab_path}\")\n    print(f\"âœ… åˆå¹¶åˆ—è¡¨å·²ä¿å­˜è‡³: {merges_path}\")\n\n    # å†…å­˜åˆ†æ\n    import psutil\n    process = psutil.Process()\n    mem_usage = process.memory_info().rss / (1024 ** 3)  # GB\n    print(f\"ğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: {mem_usage:.2f} GB\")","outputs":[{"output_type":"stream","name":"stdout","text":"ğŸš€ å¼€å§‹è®­ç»ƒ\nğŸ“– ä» /home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt åŠ è½½å¹¶é‡‡æ · 22000 ä¸ªæ–‡æ¡£...\n"},{"output_type":"stream","name":"stderr","text":"é¢„åˆ†è¯: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22000/22000 [00:03<00:00, 5957.95it/s]\n"},{"output_type":"stream","name":"stdout","text":"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° 4,287,540 ä¸ªtokenåºåˆ—\nğŸ”§ æ„å»ºBPEç´¢å¼•...\nğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡åˆå¹¶æ•°: 9,741\n"},{"output_type":"stream","name":"stderr","text":"è®­ç»ƒBPE: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9741/9741 [54:27<00:00,  2.98åˆå¹¶/s]  \n"},{"output_type":"stream","name":"stdout","text":"\nâœ… è®­ç»ƒå®Œæˆ! è€—æ—¶: 3281.00ç§’\n\nğŸ”¬ å°è§„æ¨¡éªŒè¯\nğŸ“– ä» /home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-valid.txt åŠ è½½å¹¶é‡‡æ · 2 ä¸ªæ–‡æ¡£...\n"},{"output_type":"stream","name":"stderr","text":"é¢„åˆ†è¯: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 584.98it/s]\n"},{"output_type":"stream","name":"stdout","text":"âœ… é¢„åˆ†è¯å®Œæˆï¼Œå¾—åˆ° 600 ä¸ªtokenåºåˆ—\nğŸ”§ æ„å»ºBPEç´¢å¼•...\nğŸ”„ å¼€å§‹BPEè®­ç»ƒï¼Œç›®æ ‡åˆå¹¶æ•°: 9,741\n"},{"output_type":"stream","name":"stderr","text":"è®­ç»ƒBPE:   3%|â–         | 284/9741 [00:00<00:00, 9861.68åˆå¹¶/s]"},{"output_type":"stream","name":"stdout","text":"\nâš ï¸ æ²¡æœ‰æ›´å¤šæœ‰æ•ˆçš„å­—ç¬¦å¯¹å¯ä¾›åˆå¹¶ï¼Œæå‰ç»“æŸè®­ç»ƒ\n\nğŸ“Š è®­ç»ƒç»“æœ\nè®­ç»ƒè¯æ±‡è¡¨å¤§å°: 10,000\nè®­ç»ƒåˆå¹¶æ“ä½œæ•°: 9,741\néªŒè¯è¯æ±‡è¡¨å¤§å°: 543\néªŒè¯åˆå¹¶æ“ä½œæ•°: 284\n\nğŸ“ˆ è¯æ±‡è¡¨é‡å ç‡: 5.4%\n\nğŸ” åˆ†è¯å™¨è¯„ä¼°\næ ·ä¾‹æ–‡æœ¬: u don't have to be scared of the loud dog, I'll protect you\". The mole felt so safe with the little girl. She was very kind and the mole soon came to trust her. He leaned against her and she kept him ...\nè¯æ±‡è¡¨å¤§å°: 10,000\nå”¯ä¸€tokenæ•°: 10,000\nåˆå¹¶æ“ä½œæ•°: 9,741\nâœ… è¯æ±‡è¡¨å·²ä¿å­˜è‡³: /home/mw/project/gpt2_vocab.json\nâœ… åˆå¹¶åˆ—è¡¨å·²ä¿å­˜è‡³: /home/mw/project/gpt2_merges.txt\nğŸ’¾ å³°å€¼å†…å­˜ä½¿ç”¨: 3.87 GB\n"},{"output_type":"stream","name":"stderr","text":"\n"}],"execution_count":1},{"cell_type":"markdown","metadata":{"id":"F99E977FD0894846B2AF5F90122AECB3","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"\n| æ“ä½œ | ä¼ ç»Ÿ BPE | BPEIndex | ä¸ºä»€ä¹ˆæ˜¯ O(N log N) |  \n|------|----------|----------|---------------------|  \n| **åˆå§‹åŒ–** | æ—  | O(N) | ä¸€æ¬¡æ€§æ‰«æ |  \n| **æ¯æ¬¡æ‰¾æœ€é«˜é¢‘å¯¹** | O(N) | **O(1)** | å †é¡¶ç›´æ¥å– |  \n| **æ¯æ¬¡åˆå¹¶æ›´æ–°** | O(N) | **O(k log N)** | k=å—å½±å“ä½ç½®æ•° |  \n| **æ€»å¤æ‚åº¦** | **O(NÂ²)** | **O(N log N)** | å…³é”®å·®å¼‚ç‚¹ |"},{"cell_type":"markdown","metadata":{"id":"4583477937134F11A1F3E2A0207E037A","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.6 BPE åˆ†è¯å™¨ï¼šç¼–ç å’Œè§£ç   \nåœ¨ä½œä¸šçš„å‰ä¸€éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåœ¨è¾“å…¥æ–‡æœ¬ä¸Šè®­ç»ƒ BPE åˆ†è¯å™¨ï¼Œä»¥è·å¾—åˆ†è¯å™¨è¯æ±‡è¡¨å’Œ BPE åˆå¹¶åˆ—è¡¨ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ª BPE åˆ†è¯å™¨ï¼Œå®ƒåŠ è½½æä¾›çš„è¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç å’Œè§£ç åˆ°æ ‡è®° ID æˆ–ä»æ ‡è®° ID ä¸­è¿›è¡Œç¼–ç å’Œè§£ç ã€‚  \n### 2.6.1 å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç   \n\nBPE å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç çš„è¿‡ç¨‹åæ˜ äº†æˆ‘ä»¬è®­ç»ƒ BPE è¯æ±‡çš„æ–¹å¼ã€‚æ•´ä¸ªè¿‡ç¨‹å¯åˆ†ä¸ºä»¥ä¸‹å‡ ä¸ªä¸»è¦æ­¥éª¤ï¼š  \n\n#### ç¬¬ 1 æ­¥ï¼šé¢„åˆ†è¯åŒ–ï¼ˆPre-tokenizationï¼‰  \n\næˆ‘ä»¬é¦–å…ˆä½¿ç”¨é¢„åˆ†è¯å™¨ï¼ˆpre-tokenizerï¼‰å°†è¾“å…¥æ–‡æœ¬åˆ‡åˆ†ä¸ºâ€œé¢„åˆ†è¯â€ï¼ˆpre-tokensï¼‰ã€‚å¸¸è§çš„ç­–ç•¥æ˜¯æŒ‰ç©ºæ ¼æˆ–æ ‡ç‚¹åˆ‡åˆ†ï¼Œä½†ä¿ç•™è¾¹ç•Œä¿¡æ¯ï¼ˆä¾‹å¦‚ï¼Œå°†ç©ºæ ¼ä½œä¸ºä¸‹ä¸€ä¸ªè¯çš„å‰ç¼€ï¼‰ã€‚æ¯ä¸ªé¢„åˆ†è¯å°†è¢«ç‹¬ç«‹å¤„ç†ã€‚  \n\nç„¶åï¼Œæˆ‘ä»¬å°†æ¯ä¸ªé¢„åˆ†è¯è½¬æ¢ä¸ºå…¶å¯¹åº”çš„ UTF-8 å­—èŠ‚åºåˆ—ï¼ˆbytesï¼‰ï¼Œä½œä¸ºåç»­åˆå¹¶æ“ä½œçš„åŸºæœ¬å•ä½ã€‚  \n\n> ğŸ” ç¤ºä¾‹ï¼šå­—ç¬¦ä¸² `'the cat ate'` è¢«åˆ‡åˆ†ä¸º `['the', ' cat', ' ate']`ï¼Œæ¯ä¸ªå…ƒç´ éƒ½ä»¥å­—èŠ‚å½¢å¼è¡¨ç¤ºã€‚  \n\n#### ç¬¬ 2 æ­¥ï¼šåº”ç”¨ BPE åˆå¹¶è§„åˆ™  \n\nå¯¹äºæ¯ä¸ªé¢„åˆ†è¯ï¼Œæˆ‘ä»¬å°†å…¶åˆå§‹å­—èŠ‚åºåˆ—æŒ‰ç…§è®­ç»ƒé˜¶æ®µå­¦åˆ°çš„ **åˆå¹¶è§„åˆ™åˆ—è¡¨ï¼ˆmergesï¼‰** é€æ­¥åˆå¹¶ã€‚åˆå¹¶é¡ºåºè‡³å…³é‡è¦â€”â€”å¿…é¡»ä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ—¶äº§ç”Ÿçš„é¡ºåºä¾æ¬¡å°è¯•ã€‚  \n\n- æ¯æ¬¡æŸ¥æ‰¾å½“å‰åºåˆ—ä¸­æ˜¯å¦å­˜åœ¨å¯åº”ç”¨çš„åˆå¹¶å¯¹ï¼ˆç›¸é‚»ä¸”å®Œå…¨åŒ¹é…ï¼‰  \n- è‹¥å­˜åœ¨ï¼Œåˆ™æ‰§è¡Œåˆå¹¶ï¼Œç”Ÿæˆæ›´é•¿çš„å­è¯å•å…ƒ  \n- é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°æ— æ³•å†åº”ç”¨ä»»ä½•è§„åˆ™  \n\nâš ï¸ æ³¨æ„ï¼š**åˆå¹¶ä¸ä¼šè·¨è¶Šé¢„åˆ†è¯è¾¹ç•Œ**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸åŒé¢„åˆ†è¯ä¹‹é—´çš„å­—èŠ‚ä¸ä¼šè¢«åˆå¹¶ï¼Œä¿è¯äº†åˆ†è¯çš„å±€éƒ¨æ€§ä¸å¯é¢„æµ‹æ€§ã€‚  \n\n#### ç¬¬ 3 æ­¥ï¼šæ˜ å°„ä¸º Token ID  \n\nå½“æ¯ä¸ªé¢„åˆ†è¯å®Œæˆæ‰€æœ‰å¯èƒ½çš„åˆå¹¶åï¼Œå¾—åˆ°ä¸€ç»„æœ€ç»ˆçš„å­è¯å•å…ƒï¼ˆsubword unitsï¼‰ã€‚æˆ‘ä»¬é€šè¿‡æŸ¥è¡¨çš„æ–¹å¼ï¼Œå°†è¿™äº›å­è¯å•å…ƒæ˜ å°„ä¸ºè¯æ±‡è¡¨ä¸­çš„æ•´æ•° IDï¼Œå½¢æˆæœ€ç»ˆçš„ token ID åºåˆ—ã€‚  \n\n---  \n\n### 2.6.2 è¯¦ç»†æ¡ˆä¾‹è§£æï¼š`'the cat ate'` çš„å®Œæ•´ç¼–ç è¿‡ç¨‹  \n\nä¸ºäº†æ›´æ·±å…¥ç†è§£ä¸Šè¿°æµç¨‹ï¼Œä¸‹é¢æˆ‘ä»¬å¯¹è¾“å…¥å­—ç¬¦ä¸² `'the cat ate'` è¿›è¡Œç«¯åˆ°ç«¯çš„ BPE ç¼–ç æ¼”ç¤ºã€‚  \n\n#### è¾“å…¥ä¿¡æ¯  \n\n- **è¾“å…¥å­—ç¬¦ä¸²**ï¼š`'the cat ate'`  \n- **è¯æ±‡è¡¨ï¼ˆVocabularyï¼‰**ï¼š  \n  ```python  \n  {  \n    0: b' ',     # ç©ºæ ¼  \n    1: b'a',  \n    2: b'c',  \n    3: b'e',  \n    4: b'h',  \n    5: b't',  \n    6: b'th',  \n    7: b' c',   # ç©ºæ ¼ + c  \n    8: b' a',   # ç©ºæ ¼ + a  \n    9: b'the',  \n   10: b' at'   # ç©ºæ ¼ + a + t  \n  }  \n  ```\n\n- **åˆå¹¶è§„åˆ™ï¼ˆMergesï¼‰**ï¼ˆæŒ‰ä¼˜å…ˆçº§é¡ºåºï¼‰ï¼š  \n  ```python  \n  [  \n    (b't', b'h'),      # â†’ b'th'  \n    (b' ', b'c'),      # â†’ b' c'  \n    (b' ', b'a'),      # â†’ b' a'  \n    (b'th', b'e'),     # â†’ b'the'  \n    (b' a', b't')      # â†’ b' at'  \n  ]  \n  ```\n\n- **é¢„åˆ†è¯ç­–ç•¥**ï¼šæŒ‰ç©ºæ ¼åˆ†å‰²ï¼Œç©ºæ ¼å½’å±äºåç»­è¯ï¼ˆå³ä½œä¸ºå‰ç¼€ï¼‰  \n\n---  \n\n#### æ­¥éª¤ä¸€ï¼šé¢„åˆ†è¯ï¼ˆPre-tokenizationï¼‰  \n\nåŸå§‹å­—ç¬¦ä¸²ï¼š`'the cat ate'`  \n\nåˆ‡åˆ†ç»“æœï¼š  \n```python  \n['the', ' cat', ' ate']  \n```\n\nè§£é‡Šï¼š  \n- `'the'`ï¼šæ— å‰å¯¼ç©ºæ ¼  \n- `' cat'`ï¼šåŒ…å«å‰å¯¼ç©ºæ ¼  \n- `' ate'`ï¼šåŒ…å«å‰å¯¼ç©ºæ ¼  \n\n> ğŸ’¡ æ­¤ç­–ç•¥å½±å“åç»­åˆå¹¶è¡Œä¸ºï¼Œå› ä¸ºç©ºæ ¼è¢«è§†ä¸ºå­—ç¬¦çš„ä¸€éƒ¨åˆ†ã€‚  \n\n---  \n\n#### æ­¥éª¤äºŒï¼šé€ä¸ªé¢„åˆ†è¯åº”ç”¨ BPE åˆå¹¶  \n\n##### âœ… é¢„åˆ†è¯ 1: `'the'`  \n\n1. **åˆå§‹åˆ†è§£**ï¼š`[b't', b'h', b'e']`  \n2. **åº”ç”¨åˆå¹¶**ï¼š  \n   - `(b't', b'h')` â†’ åˆå¹¶ä¸º `[b'th', b'e']`  \n   - `(b'th', b'e')` â†’ åˆå¹¶ä¸º `[b'the']`  \n3. **æŸ¥è¡¨å¾— ID**ï¼š`b'the'` â†’ ID `9`  \n4. **è¾“å‡º**ï¼š`[9]`  \n\nâœ… å®Œæˆã€‚  \n\n---  \n\n##### âœ… é¢„åˆ†è¯ 2: `' cat'`  \n\n1. **åˆå§‹åˆ†è§£**ï¼š`[b' ', b'c', b'a', b't']`  \n2. **åº”ç”¨åˆå¹¶**ï¼š  \n   - `(b' ', b'c')` â†’ åˆå¹¶ä¸º `[b' c', b'a', b't']`  \n   - å…¶ä»–è§„åˆ™æ— æ³•åº”ç”¨ï¼š  \n     - `(b' ', b'a')`ï¼šå½“å‰æ²¡æœ‰ç‹¬ç«‹çš„ `b' '` å’Œ `b'a'` ç›¸é‚»  \n     - `(b' a', b't')`ï¼šéœ€è¦ `b' a'`ï¼Œä½†æ­¤å¤„æ˜¯ `b'a'` ä¸”å‰æ¥ `b' c'`  \n3. **æŸ¥è¡¨å¾— ID**ï¼š  \n   - `b' c'` â†’ 7  \n   - `b'a'` â†’ 1  \n   - `b't'` â†’ 5  \n4. **è¾“å‡º**ï¼š`[7, 1, 5]`  \n\nâœ… å®Œæˆã€‚  \n\n---  \n\n##### âœ… é¢„åˆ†è¯ 3: `' ate'`  \n\n1. **åˆå§‹åˆ†è§£**ï¼š`[b' ', b'a', b't', b'e']`  \n2. **åº”ç”¨åˆå¹¶**ï¼š  \n   - `(b' ', b'a')` â†’ åˆå¹¶ä¸º `[b' a', b't', b'e']`  \n   - `(b' a', b't')` â†’ åˆå¹¶ä¸º `[b' at', b'e']`  \n3. **æŸ¥è¡¨å¾— ID**ï¼š  \n   - `b' at'` â†’ 10  \n   - `b'e'` â†’ 3  \n4. **è¾“å‡º**ï¼š`[10, 3]`  \n\nâœ… å®Œæˆã€‚  \n\n---  \n\n#### æ­¥éª¤ä¸‰ï¼šæ‹¼æ¥æ‰€æœ‰ token ID  \n\nå°†å„é¢„åˆ†è¯çš„ç¼–ç ç»“æœä¸²è”ï¼š  \n\n- `'the'` â†’ `[9]`  \n- `' cat'` â†’ `[7, 1, 5]`  \n- `' ate'` â†’ `[10, 3]`  \n\næœ€ç»ˆç¼–ç ç»“æœï¼š  \n```python  \n[9, 7, 1, 5, 10, 3]  \n```\n\n---  \n\n### 2.6.3 å…³é”®è¦ç‚¹æ€»ç»“  \n\n| è¦ç‚¹ | è¯´æ˜ |  \n|------|------|  \n| ğŸ” **åˆå¹¶é¡ºåºå†³å®šç»“æœ** | å¿…é¡»ä¸¥æ ¼æŒ‰ç…§è®­ç»ƒæ—¶ç”Ÿæˆçš„é¡ºåºå°è¯•åˆå¹¶ï¼Œé¡ºåºä¸åŒå¯èƒ½å¯¼è‡´ä¸åŒè¾“å‡º |  \n| ğŸš« **ä¸è·¨é¢„åˆ†è¯è¾¹ç•Œåˆå¹¶** | å³ä½¿ä¸¤ä¸ªé¢„åˆ†è¯æœ«å°¾å’Œå¼€å¤´å¯ä»¥åˆå¹¶ï¼ˆå¦‚ `'the'` å’Œ `' cat'` ä¸­çš„ `'e'` å’Œ `' '`ï¼‰ï¼Œä¹Ÿä¸ä¼šå‘ç”Ÿè·¨è¯åˆå¹¶ |  \n| âš ï¸ **ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§** | ç›¸åŒå­—ç¬¦ç»„åˆå› ä¸Šä¸‹æ–‡ä¸åŒå¯èƒ½è¢«ä¸åŒç¼–ç ï¼ˆå¦‚ `'at'` åœ¨ `'cat'` å’Œ `' at'` ä¸­è¡¨ç°ä¸åŒï¼‰ |  \n| ğŸ§© **ç©ºæ ¼å¤„ç†æ–¹å¼è‡³å…³é‡è¦** | æ˜¯å¦ä¿ç•™ã€ä½œä¸ºå‰ç¼€/åç¼€ï¼Œç›´æ¥å½±å“åˆå¹¶è·¯å¾„ |  \n| ğŸ“‡ **è¯æ±‡è¡¨å®Œæ•´æ€§** | æ‰€æœ‰åˆå¹¶åçš„å­è¯å¿…é¡»å­˜åœ¨äºè¯æ±‡è¡¨ä¸­ï¼Œå¦åˆ™æ— æ³•æ˜ å°„ä¸º ID |  \n\n---  \n\n"},{"cell_type":"code","metadata":{"id":"3034787CB08D4E43BB08B5A41A6BCCB5","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install regex","outputs":[{"output_type":"stream","name":"stdout","text":"Collecting regex\n  Downloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nDownloading regex-2025.7.34-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (801 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m801.9/801.9 kB\u001b[0m \u001b[31m170.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:02\u001b[0m\n\u001b[?25hInstalling collected packages: regex\nSuccessfully installed regex-2025.7.34\n"}],"execution_count":3},{"cell_type":"markdown","metadata":{"id":"F4EAE38502B5498DA4CCAF7FDD0C1B83","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### é—®é¢˜ï¼ˆtrain_bpe_expts_owtï¼‰ï¼šOpenWebText ä¸Šçš„ BPE è®­ç»ƒï¼ˆ2 åˆ†ï¼‰  \n(a) åœ¨ OpenWebText æ•°æ®é›†ä¸Šè®­ç»ƒä¸€ä¸ªå­—èŠ‚çº§ BPE åˆ†è¯å™¨ï¼Œæœ€å¤§è¯æ±‡è¡¨å¤§å°ä¸º 32,000ã€‚å°†ç”Ÿæˆçš„è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™åºåˆ—åŒ–ä¿å­˜åˆ°ç£ç›˜ä»¥ä¾¿è¿›ä¸€æ­¥æ£€æŸ¥ã€‚è¯æ±‡è¡¨ä¸­æœ€é•¿çš„ token æ˜¯ä»€ä¹ˆï¼Ÿè¿™åˆç†å—ï¼Ÿ  \nèµ„æºè¦æ±‚ï¼šâ‰¤ 12 å°æ—¶ï¼ˆä¸ä½¿ç”¨ GPUï¼‰ï¼Œâ‰¤ 100GB å†…å­˜  \näº¤ä»˜å†…å®¹ï¼šä¸€åˆ°ä¸¤å¥è¯çš„å›å¤ã€‚  \n\n(b) æ¯”è¾ƒå¹¶å¯¹æ¯”åœ¨ TinyStories æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„åˆ†è¯å™¨ä¸åœ¨ OpenWebText æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„åˆ†è¯å™¨ã€‚  \näº¤ä»˜å†…å®¹ï¼šä¸€åˆ°ä¸¤å¥è¯çš„å›å¤ã€‚  \n\n"},{"cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4FC61B519BE24DD590C9913A15FBDA95","scrolled":false,"notebookId":"688ef4fd01e7980f7a117804"},"source":"print(\"hhh\")","outputs":[{"output_type":"stream","name":"stdout","text":"hhh\n"}],"execution_count":5},{"cell_type":"markdown","metadata":{"id":"EC78CDE421124C5FAB1DBFB6814853B0","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### 2.6.4 è§£ç ï¼šä» Token ID åˆ°æ–‡æœ¬  \n\nç¼–ç çš„é€†è¿‡ç¨‹ç§°ä¸º**è§£ç **ï¼ˆDecodingï¼‰ï¼Œå³å°† token ID åºåˆ—è¿˜åŸä¸ºåŸå§‹æ–‡æœ¬ã€‚  \n\n#### è§£ç æ­¥éª¤ï¼š  \n\n1. **æŸ¥è¡¨åå‘æ˜ å°„**ï¼šå°†æ¯ä¸ª ID è½¬æ¢å›å¯¹åº”çš„å­—èŠ‚ä¸²ï¼ˆå¦‚ `9 â†’ b'the'`ï¼‰  \n2. **æ‹¼æ¥å­—èŠ‚ä¸²**ï¼šæŒ‰é¡ºåºè¿æ¥æ‰€æœ‰å­—èŠ‚  \n3. **è§£ç ä¸ºå­—ç¬¦ä¸²**ï¼šå°†æ‹¼æ¥åçš„å­—èŠ‚åºåˆ—ç”¨ UTF-8 è§£ç ä¸º Unicode å­—ç¬¦ä¸²  \n4. **å¤„ç†ç©ºæ ¼è§„èŒƒåŒ–ï¼ˆå¯é€‰ï¼‰**ï¼šæ ¹æ®éœ€è¦å»é™¤å¤šä½™ç©ºæ ¼æˆ–è°ƒæ•´æ ¼å¼  \n\n#### ç¤ºä¾‹ï¼šè§£ç  `[9, 7, 1, 5, 10, 3]`  \n\n1. æŸ¥è¡¨ï¼š  \n   - 9 â†’ `b'the'`  \n   - 7 â†’ `b' c'`  \n   - 1 â†’ `b'a'`  \n   - 5 â†’ `b't'`  \n   - 10 â†’ `b' at'`  \n   - 3 â†’ `b'e'`  \n\n2. æ‹¼æ¥å­—èŠ‚ï¼š  \n   `b'the' + b' c' + b'a' + b't' + b' at' + b'e'`  "},{"cell_type":"markdown","metadata":{"id":"9E87E5081F464E3EB843FFDC610CA994","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"### é—®é¢˜ï¼šå®ç°åˆ†è¯å™¨ï¼ˆ15åˆ†ï¼‰  \n\nå¯äº¤ä»˜æˆæœï¼šå®ç°ä¸€ä¸ª Tokenizer ç±»ï¼Œè¯¥ç±»åœ¨ç»™å®šè¯æ±‡è¡¨å’Œåˆå¹¶åˆ—è¡¨çš„æƒ…å†µä¸‹ï¼Œå°†æ–‡æœ¬ç¼–ç ä¸ºæ•´æ•° IDï¼Œå¹¶å°†æ•´æ•° ID è§£ç ä¸ºæ–‡æœ¬ã€‚åˆ†è¯å™¨è¿˜åº”æ”¯æŒç”¨æˆ·æä¾›çš„ç‰¹æ®Šä»¤ç‰Œï¼ˆå¦‚æœè¯æ±‡è¡¨å°šä¸å­˜åœ¨ï¼Œåˆ™å°†å…¶é™„åŠ åˆ°è¯æ±‡è¡¨ä¸­ï¼‰ã€‚æˆ‘ä»¬æ¨èä»¥ä¸‹ç•Œé¢ï¼š  \n`def __init__(self, vocab, merges, special_tokens=None)`  \næ ¹æ®ç»™å®šçš„è¯æ±‡è¡¨ã€åˆå¹¶è§„åˆ™åˆ—è¡¨ä»¥åŠï¼ˆå¯é€‰çš„ï¼‰ç‰¹æ®Šæ ‡è®°åˆ—è¡¨æ„å»ºä¸€ä¸ªåˆ†è¯å™¨ã€‚è¯¥å‡½æ•°åº”æ¥å—ä»¥ä¸‹å‚æ•°ï¼š  \n\n- `vocab`: ä¸€ä¸ªä»æ•´æ•° ID æ˜ å°„åˆ°å­—èŠ‚ä¸²çš„å­—å…¸ï¼ˆdict[int, bytes]ï¼‰  \n- `merges`: ä¸€ä¸ªåŒ…å«å­—èŠ‚å¯¹å…ƒç»„çš„åˆ—è¡¨ï¼Œè¡¨ç¤º BPE åˆå¹¶è§„åˆ™ï¼ˆlist[tuple[bytes, bytes]]ï¼‰  \n- `special_tokens`: ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œå¯é€‰ï¼ˆlist[str] | None = Noneï¼‰  \n\n`def from_files(cls, vocab_filepath, merges_filepath, special_tokens=None)`  \nä¸€ä¸ªç±»æ–¹æ³•ï¼Œç”¨äºä»åºåˆ—åŒ–çš„è¯æ±‡è¡¨æ–‡ä»¶å’Œåˆå¹¶è§„åˆ™æ–‡ä»¶ï¼ˆæ ¼å¼ä¸ä½ è®­ç»ƒ BPE ä»£ç è¾“å‡ºçš„æ ¼å¼ç›¸åŒï¼‰æ„å»ºå¹¶è¿”å›ä¸€ä¸ª Tokenizerï¼ŒåŒæ—¶å¯é€‰åœ°æ¥æ”¶ä¸€ä¸ªç‰¹æ®Šæ ‡è®°åˆ—è¡¨ã€‚è¯¥æ–¹æ³•åº”æ¥å—ä»¥ä¸‹é¢å¤–å‚æ•°ï¼š  \n\n- `vocab_filepath`: è¯æ±‡è¡¨æ–‡ä»¶çš„è·¯å¾„ï¼ˆstrï¼‰  \n- `merges_filepath`: åˆå¹¶è§„åˆ™æ–‡ä»¶çš„è·¯å¾„ï¼ˆstrï¼‰  \n- `special_tokens`: ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ï¼Œè¡¨ç¤ºç‰¹æ®Šæ ‡è®°ï¼Œå¯é€‰ï¼ˆlist[str] | None = Noneï¼‰  \n\n`def encode(self, text: str) -> list[int]`  \nå°†è¾“å…¥æ–‡æœ¬ç¼–ç ä¸ºä¸€ä¸ª token ID åºåˆ—ã€‚  \n\n`def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]`  \nç»™å®šä¸€ä¸ªå­—ç¬¦ä¸²çš„å¯è¿­ä»£å¯¹è±¡ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ª Python æ–‡ä»¶å¥æŸ„ï¼‰ï¼Œè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæƒ°æ€§åœ°é€ä¸ªç”Ÿæˆ token IDã€‚è¿™å¯¹äºå†…å­˜å—é™æƒ…å†µä¸‹å¯¹å¤§å‹æ–‡ä»¶è¿›è¡Œé«˜æ•ˆåˆ†è¯æ˜¯å¿…éœ€çš„ã€‚  \n\n`def decode(self, ids: list[int]) -> str`  \nå°†ä¸€ä¸ª token ID åºåˆ—è§£ç ä¸ºåŸå§‹æ–‡æœ¬ã€‚è¦ä½¿ç”¨æˆ‘ä»¬æä¾›çš„æµ‹è¯•ç”¨ä¾‹éªŒè¯ä½ çš„ Tokenizer å®ç°ï¼Œä½ éœ€è¦å…ˆå®ç° [adapters.get_tokenizer] ä¸­çš„æµ‹è¯•é€‚é…å™¨ï¼Œç„¶åè¿è¡Œ `uv run pytest tests/test_tokenizer.py`ã€‚ä½ çš„å®ç°åº”èƒ½é€šè¿‡æ‰€æœ‰æµ‹è¯•ã€‚"},{"cell_type":"code","metadata":{"id":"0F82E74F672F44A4AB4C93ABFB4B605E","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install tiktoken -i https://pypi.tuna.tsinghua.edu.cn/simple","outputs":[{"output_type":"stream","name":"stdout","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting tiktoken\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9a/a8/84ad6c23cfd51d2c39ba1bfbc70b6208c6be3e5cd15132a9d11cc93bfdd5/tiktoken-0.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.12/site-packages (from tiktoken) (2.32.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\nInstalling collected packages: tiktoken\nSuccessfully installed tiktoken-0.10.0\n"}],"execution_count":3},{"cell_type":"code","metadata":{"id":"B35F48CEF53F4E898392185DB78CA8CD","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"!pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple","outputs":[{"output_type":"stream","name":"stdout","text":"Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nCollecting torch\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/99/a8/6acf48d48838fb8fe480597d98a0668c2beb02ee4755cc136de92a0a956f/torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting filelock (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl (16 kB)\nRequirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.13.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.1.0)\nRequirement already satisfied: sympy>=1.13.3 in /opt/conda/lib/python3.12/site-packages (from torch) (1.14.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch) (2025.3.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/05/6b/32f747947df2da6994e999492ab306a903659555dddc0fbdeb9d71f75e52/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0d/9b/a997b638fcd068ad6e4d53b8551a7d30fe8b404d6f1804abf1df69838932/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f8/02/2adcaa145158bf1a8295d83591d22e4103dbfd821bcaf6f3f53151ca4ffa/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/51/e123d997aa098c61d029f76663dedbfb9bc8dcf8c60cbd6adbe42f76d049/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/dc/61/e24b560ab2e2eaeb3c839129175fb330dfcfc29e5203196e5541a4c44682/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/1f/13/ee4e00f30e676b66ae65b4f08cb5bcbb8392c03f54f2d5413ea99a5d1c80/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.9.90 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fb/aa/6584b56dc84ebe9cf93226a5cde4d99080c8e90ab40f0c27bda7a0f29aa1/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/85/48/9a13d2975803e8cf2777d5ed57b87a0b6ca2cc795f9a4f59796a910bfb80/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c2/f5/e1854cb2f2bcd4280c44736c93550cc300ff4b8c95ebe370d0aa7d2b473d/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/56/79/12978b96bd44274fe38b5dde5cfb660b1d114f70a65ef962bcbbed99b549/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu12==2.27.3 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/5c/5b/4e4fff7bad39adf89f735f2bc87248c81db71205b62bcc0d5ca5b606b3c3/nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu12==12.8.90 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a2/eb/86626c1bbc2edb86323022371c39aa48df6fd8b0a1647bc274577f72e90b/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\nCollecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f6/74/86a07f1d0f42998ca31312f998bd3b9a7eff7f52378f4f270c8679c77fb9/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/bb/fe/1bcba1dfbfb8d01be8d93f07bfc502c93fa23afa6fd5ab3fc7c1df71038a/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==3.4.0 (from torch)\n  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d0/66/b1eb52839f563623d185f0927eb3530ee4d5ffe9d377cdaf5346b306689e/triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.6 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.6/155.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\nInstalling collected packages: nvidia-cusparselt-cu12, triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n\u001b[2K  Attempting uninstall: nvidia-nccl-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/17\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.5â”â”â”â”â”â”\u001b[0m \u001b[32m 3/17\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.5:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/17\u001b[0m [nvidia-nvjitlink-cu12]\n\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.5â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 4/17\u001b[0m [nvidia-nccl-cu12]]\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17/17\u001b[0m [torch]m16/17\u001b[0m [torch]-cusolver-cu12]2]2]\n\u001b[1A\u001b[2KSuccessfully installed filelock-3.18.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.8.0 triton-3.4.0\n"}],"execution_count":2},{"cell_type":"code","metadata":{"id":"383EE623B8B94E6A941D8186089B7499","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import regex\nfrom collections import defaultdict\nfrom typing import Iterable, Iterator, List, Set, Tuple\nimport torch\nPAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n\nclass Tokenizer:\n    def __init__(self, vocab, merges, special_tokens=None):\n        self.vocab = vocab\n        self.merges = merges\n        self.special_tokens = special_tokens or []\n        # ç”±äºéœ€è¦é€šè¿‡mergeså­—å…¸æ¥æ’åºï¼Œæ‰€ä»¥éœ€è¦ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨mergesçš„ä¼˜å…ˆçº§\n        self.merges_priority_map = {pair: i for i, pair in enumerate(self.merges)}\n        # å°†å­—èŠ‚è½¬æ¢ä¸ºtoken idï¼Œé¿å…ç›´æ¥ä½¿ç”¨vocabå­—å…¸\n        self.bytes_to_id = {v: k for k, v in self.vocab.items()}\n\n\n    def _get_bpe_merges(self, piece: bytes) -> List[bytes]:\n        \"\"\"\n        å¯¹äºæ¯ä¸€ä¸ªéç‰¹æ®Šç¬¦å·çš„å­—èŠ‚æ®µwordï¼Œä¾‹å¦‚\"hello\" è¿›è¡ŒBPEç¼–ç ï¼Œè¿”å›ä¸€ä¸ªå­—èŠ‚åˆ—è¡¨\n        \"\"\"\n        # é¦–å…ˆå°†å­—èŠ‚æ®µpieceè½¬æ¢ä¸ºå•å­—èŠ‚åˆ—è¡¨\n        parts = [bytes([b]) for b in piece]\n        while len(parts) > 1:\n            # è®°å½•æ‰€æœ‰åˆå¹¶å¯¹\n            pairs = set()\n            for i in range(len(parts) - 1):\n                pair = (parts[i], parts[i+1])\n                if pair in self.merges_priority_map:\n                    pairs.add(pair)\n            \n            if not pairs:\n                break # å¦‚æœå‰©ä¸‹çš„åˆå¹¶å¯¹éƒ½ä¸åœ¨mergeså­—å…¸ä¸­ï¼Œå°±è¡¨ç¤ºæ²¡æœ‰åº”è¯¥åˆå¹¶çš„åˆå¹¶å¯¹äº†ï¼Œç›´æ¥è¿”å›\n\n            # æ‰¾åˆ°æœ€ä½³åˆå¹¶å¯¹\n            best_pair = min(pairs, key=lambda pair: self.merges_priority_map[pair])\n\n            # åº”ç”¨æœ€ä½³åˆå¹¶å¯¹\n            new_parts = []\n            i = 0\n            while i < len(parts):\n                if i < len(parts) - 1 and (parts[i], parts[i+1]) == best_pair:\n                    new_parts.append(parts[i] + parts[i+1])\n                    i += 2\n                else:\n                    new_parts.append(parts[i])\n                    i += 1\n            parts = new_parts\n        return parts\n\n    def encode(self, text: str) -> List[int]:\n        if not text:\n            return []\n\n        # åˆ›å»ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åˆ†å‰²ç‰¹æ®Šç¬¦å·\n        # æŒ‰ç…§é•¿åº¦é™åºæ’åºï¼Œç¡®ä¿æ›´é•¿çš„ç¬¦å·ï¼ˆä¾‹å¦‚\"<|eot|><|eot|>\") åœ¨æ›´çŸ­çš„ç¬¦å·ï¼ˆä¾‹å¦‚\"<|eot|>\")ä¹‹å‰è¢«åŒ¹é…\n        sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)\n        special_token_pattern = '|'.join(map(regex.escape, sorted_special_tokens))\n\n        if self.special_tokens:\n            # æŒ‰ç…§ç‰¹æ®Šç¬¦å·åˆ†å‰²textï¼Œä¿æŒç‰¹æ®Šç¬¦å·ä½œä¸ºåˆ†éš”ç¬¦\n            chunks = regex.split(f'({special_token_pattern})', text)\n        else:\n            chunks = [text]\n\n        final_ids = []\n        for chunk in chunks:\n            if not chunk:\n                continue\n\n            if chunk in self.special_tokens:\n                # å¦‚æœchunkæ˜¯ç‰¹æ®Šç¬¦å·ï¼Œç›´æ¥ç¼–ç \n                final_ids.append(self.bytes_to_id[chunk.encode('utf-8')])\n            else:\n                # å¦‚æœchunkæ˜¯æ™®é€šæ–‡æœ¬ï¼Œä½¿ç”¨BPEç®—æ³•å¤„ç†\n                # é¦–å…ˆï¼Œä½¿ç”¨PATæ­£åˆ™è¡¨è¾¾å¼å°†chunkåˆ†å‰²ä¸º\"å•è¯\"\n                for word in regex.findall(PAT, chunk):\n                    if not word:\n                        continue\n                    \n                    # è·å–wordçš„åˆå¹¶å­—èŠ‚ç‰‡æ®µ\n                    merged_pieces = self._get_bpe_merges(word.encode('utf-8'))\n                    \n                    # å°†æ¯ä¸ªç‰‡æ®µè½¬æ¢ä¸ºtoken id\n                    for piece in merged_pieces:\n                        final_ids.append(self.bytes_to_id[piece])\n        return final_ids\n\n    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n        for text in iterable:\n            yield from self.encode(text)\n\n    def decode(self, ids):\n        all_bytes = b''.join(self.vocab[id] for id in ids)\n        return all_bytes.decode(\"utf-8\", errors=\"replace\")","outputs":[],"execution_count":3},{"cell_type":"markdown","metadata":{"id":"D2F2C98CE3EA41388DC0B812023D34CA","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"è¿™é‡Œè‡ªåˆ¶ä¸€ä¸ªtestè„šæœ¬éªŒè¯"},{"cell_type":"code","metadata":{"id":"CDF80CE7E227400596BC52AD711D140B","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import regex\nfrom collections import defaultdict\nfrom typing import Iterable, Iterator, List, Set, Tuple\nimport torch\n\nPAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n\nclass Tokenizer:\n    def __init__(self, vocab, merges, special_tokens=None):\n        self.vocab = vocab\n        self.merges = merges\n        self.special_tokens = special_tokens or []\n        \n        # åˆ›å»ºä»å­—èŠ‚åˆ°IDçš„æ˜ å°„\n        self.bytes_to_id = {v: k for k, v in self.vocab.items()}\n        \n        # åˆ›å»ºç‰¹æ®Štokenåˆ°IDçš„æ˜ å°„ï¼ˆä½¿ç”¨å­—ç¬¦ä¸²è€Œä¸æ˜¯å­—èŠ‚ï¼‰\n        self.special_to_id = {}\n        for token in self.special_tokens:\n            token_bytes = token.encode('utf-8')\n            # æŸ¥æ‰¾ç‰¹æ®Štokenå¯¹åº”çš„ID\n            for id_val, bytes_val in self.vocab.items():\n                if bytes_val == token_bytes:\n                    self.special_to_id[token] = id_val\n                    break\n        \n        # ç”±äºéœ€è¦é€šè¿‡mergeså­—å…¸æ¥æ’åºï¼Œæ‰€ä»¥éœ€è¦ä¸€ä¸ªå­—å…¸æ¥å­˜å‚¨mergesçš„ä¼˜å…ˆçº§\n        self.merges_priority_map = {pair: i for i, pair in enumerate(self.merges)}\n\n    def _get_bpe_merges(self, piece: bytes) -> List[bytes]:\n        \"\"\"\n        å¯¹äºæ¯ä¸€ä¸ªéç‰¹æ®Šç¬¦å·çš„å­—èŠ‚æ®µwordï¼Œä¾‹å¦‚\"hello\" è¿›è¡ŒBPEç¼–ç ï¼Œè¿”å›ä¸€ä¸ªå­—èŠ‚åˆ—è¡¨\n        \"\"\"\n        # é¦–å…ˆå°†å­—èŠ‚æ®µpieceè½¬æ¢ä¸ºå•å­—èŠ‚åˆ—è¡¨\n        parts = [bytes([b]) for b in piece]\n        while len(parts) > 1:\n            # è®°å½•æ‰€æœ‰åˆå¹¶å¯¹\n            pairs = set()\n            for i in range(len(parts) - 1):\n                pair = (parts[i], parts[i+1])\n                if pair in self.merges_priority_map:\n                    pairs.add(pair)\n            \n            if not pairs:\n                break  # å¦‚æœå‰©ä¸‹çš„åˆå¹¶å¯¹éƒ½ä¸åœ¨mergeså­—å…¸ä¸­ï¼Œå°±è¡¨ç¤ºæ²¡æœ‰åº”è¯¥åˆå¹¶çš„åˆå¹¶å¯¹äº†ï¼Œç›´æ¥è¿”å›\n\n            # æ‰¾åˆ°æœ€ä½³åˆå¹¶å¯¹\n            best_pair = min(pairs, key=lambda pair: self.merges_priority_map[pair])\n\n            # åº”ç”¨æœ€ä½³åˆå¹¶å¯¹\n            new_parts = []\n            i = 0\n            while i < len(parts):\n                if i < len(parts) - 1 and (parts[i], parts[i+1]) == best_pair:\n                    new_parts.append(parts[i] + parts[i+1])\n                    i += 2\n                else:\n                    new_parts.append(parts[i])\n                    i += 1\n            parts = new_parts\n        return parts\n\n    def encode(self, text: str) -> List[int]:\n        if not text:\n            return []\n\n        # åˆ›å»ºä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æ¥åˆ†å‰²ç‰¹æ®Šç¬¦å·\n        # æŒ‰ç…§é•¿åº¦é™åºæ’åºï¼Œç¡®ä¿æ›´é•¿çš„ç¬¦å·åœ¨æ›´çŸ­çš„ç¬¦å·ä¹‹å‰è¢«åŒ¹é…\n        sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)\n        special_token_pattern = '|'.join(map(regex.escape, sorted_special_tokens))\n\n        if self.special_tokens:\n            # æŒ‰ç…§ç‰¹æ®Šç¬¦å·åˆ†å‰²textï¼Œä¿æŒç‰¹æ®Šç¬¦å·ä½œä¸ºåˆ†éš”ç¬¦\n            chunks = regex.split(f'({special_token_pattern})', text)\n        else:\n            chunks = [text]\n\n        final_ids = []\n        for chunk in chunks:\n            if not chunk:\n                continue\n\n            if chunk in self.special_tokens:\n                # å¦‚æœchunkæ˜¯ç‰¹æ®Šç¬¦å·ï¼Œç›´æ¥ä½¿ç”¨ç‰¹æ®Štokenæ˜ å°„\n                if chunk in self.special_to_id:\n                    final_ids.append(self.special_to_id[chunk])\n                else:\n                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹æ®Štokençš„IDï¼Œå°è¯•ä½¿ç”¨å­—èŠ‚æŸ¥æ‰¾\n                    chunk_bytes = chunk.encode('utf-8')\n                    if chunk_bytes in self.bytes_to_id:\n                        final_ids.append(self.bytes_to_id[chunk_bytes])\n                    else:\n                        # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ï¼Œä½¿ç”¨æœªçŸ¥token\n                        if '<unk>' in self.special_to_id:\n                            final_ids.append(self.special_to_id['<unk>'])\n                        else:\n                            # å¦‚æœè¿<unk>éƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹æ®Štoken\n                            final_ids.append(self.special_to_id.get(self.special_tokens[0], 0))\n            else:\n                # å¦‚æœchunkæ˜¯æ™®é€šæ–‡æœ¬ï¼Œä½¿ç”¨BPEç®—æ³•å¤„ç†\n                # é¦–å…ˆï¼Œä½¿ç”¨PATæ­£åˆ™è¡¨è¾¾å¼å°†chunkåˆ†å‰²ä¸º\"å•è¯\"\n                for word in regex.findall(PAT, chunk):\n                    if not word:\n                        continue\n                    \n                    # è·å–wordçš„åˆå¹¶å­—èŠ‚ç‰‡æ®µ\n                    merged_pieces = self._get_bpe_merges(word.encode('utf-8'))\n                    \n                    # å°†æ¯ä¸ªç‰‡æ®µè½¬æ¢ä¸ºtoken id\n                    for piece in merged_pieces:\n                        if piece in self.bytes_to_id:\n                            final_ids.append(self.bytes_to_id[piece])\n                        else:\n                            # å¤„ç†æœªçŸ¥ç‰‡æ®µ\n                            if '<unk>' in self.special_to_id:\n                                final_ids.append(self.special_to_id['<unk>'])\n                            else:\n                                # å¦‚æœè¿<unk>éƒ½æ²¡æœ‰ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹æ®Štoken\n                                final_ids.append(self.special_to_id.get(self.special_tokens[0], 0))\n        return final_ids\n\n    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n        for text in iterable:\n            yield from self.encode(text)\n\n    def decode(self, ids):\n        tokens = []\n        for id_val in ids:\n            if id_val in self.vocab:\n                tokens.append(self.vocab[id_val])\n            else:\n                # å¤„ç†æœªçŸ¥ID\n                if '<unk>' in self.special_to_id and self.special_to_id['<unk>'] in self.vocab:\n                    tokens.append(self.vocab[self.special_to_id['<unk>']])\n                else:\n                    # å¦‚æœè¿<unk>éƒ½æ²¡æœ‰ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹æ®Štoken\n                    if self.special_tokens:\n                        first_special = self.special_tokens[0].encode('utf-8')\n                        tokens.append(first_special)\n                    else:\n                        # æœ€åçš„é€‰æ‹©ï¼šä½¿ç”¨ç©ºæ ¼\n                        tokens.append(b' ')\n        \n        all_bytes = b''.join(tokens)\n        return all_bytes.decode(\"utf-8\", errors=\"replace\")\n\n# åˆ›å»ºæ¨¡æ‹Ÿè¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™\nvocab = {}\nmerges = []\n\n# æ·»åŠ å•å­—èŠ‚token (0-255)\nfor i in range(256):\n    vocab[i] = bytes([i])\n\n# æ·»åŠ åˆå¹¶è§„åˆ™å’Œåˆå¹¶åçš„token\nnext_id = 256\n\n# æ·»åŠ ç‰¹æ®Štoken\nspecial_tokens = [\"<|endoftext|>\", \"<pad>\", \"<unk>\"]\nfor token in special_tokens:\n    token_bytes = token.encode('utf-8')\n    vocab[next_id] = token_bytes\n    next_id += 1\n\n# æ·»åŠ BPEåˆå¹¶è§„åˆ™\nmerges.append((b\"h\", b\"i\"))   # hi -> 256\nmerges.append((b\"t\", b\"h\"))   # th -> 257\nmerges.append((b\"e\", b\"r\"))   # er -> 258\nmerges.append((b\"th\", b\"e\"))  # the -> 259\n\n# ä¸ºåˆå¹¶åçš„tokenåˆ†é…ID\nvocab[next_id] = b\"hi\"; next_id += 1\nvocab[next_id] = b\"th\"; next_id += 1\nvocab[next_id] = b\"er\"; next_id += 1\nvocab[next_id] = b\"the\"; next_id += 1\n\n# åˆ›å»ºtokenizerå®ä¾‹\ntokenizer = Tokenizer(\n    vocab=vocab,\n    merges=merges,\n    special_tokens=special_tokens\n)\n\n# æµ‹è¯•å­—ç¬¦ä¸²\ntext = \"the tokenizer<|endoftext|>hi there!\"\n\n# ç¼–ç ä¸ºIDåºåˆ—\nids = tokenizer.encode(text)\nprint(\"ç¼–ç åçš„IDåºåˆ—:\", ids)\n\n# è¿˜åŸIDåºåˆ—ä¸ºæ–‡æœ¬\ndecoded_text = tokenizer.decode(ids)\nprint(\"è¿˜åŸåçš„æ–‡æœ¬:\", repr(decoded_text))\n\n# éªŒè¯è¿˜åŸç»“æœ\nprint(\"è¿˜åŸæ˜¯å¦æ­£ç¡®:\", decoded_text == text)","outputs":[{"output_type":"stream","name":"stdout","text":"ç¼–ç åçš„IDåºåˆ—: [262, 32, 116, 111, 107, 101, 110, 105, 122, 261, 256, 259, 32, 260, 261, 101, 33]\nè¿˜åŸåçš„æ–‡æœ¬: 'the tokenizer<|endoftext|>hi there!'\nè¿˜åŸæ˜¯å¦æ­£ç¡®: True\n"}],"execution_count":7},{"cell_type":"code","metadata":{"id":"AD2DAA202C5644679B03F5692C28F043","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"# æ‰“å¼€æ–‡ä»¶ï¼Œè¯»å–ç¬¬ä¸‰è¡Œ\nfile_path = '/home/mw/input/cs336_129682968/TinyStoriesV2-GPT4-train.txt'\n\ntry:\n    with open(file_path, 'r', encoding='utf-8') as file:\n        lines = file.readlines()  # è¯»å–æ‰€æœ‰è¡Œ\n        if len(lines) >= 4:\n            print(\"ç¬¬ä¸‰è¡Œå†…å®¹ï¼š\")\n            print(lines[3])  # æ³¨æ„ï¼šåˆ—è¡¨ç´¢å¼•ä»0å¼€å§‹ï¼Œç¬¬3è¡Œæ˜¯ç´¢å¼•2\n        else:\n            print(\"æ–‡ä»¶ä¸è¶³ä¸‰è¡Œã€‚\")\nexcept FileNotFoundError:\n    print(f\"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}\")\nexcept Exception as e:\n    print(f\"å‘ç”Ÿé”™è¯¯: {e}\")","outputs":[{"output_type":"stream","name":"stdout","text":"ç¬¬ä¸‰è¡Œå†…å®¹ï¼š\nThe shopkeeper smiled and said, â€œOf course you can. You can take it home and show all your friends how amazing it is!â€\n\n"}],"execution_count":3},{"cell_type":"markdown","metadata":{"id":"6C273C4BA534408697311DDF5F76C5D9","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"è¿™æ˜¯æˆ‘ä»¬çš„è§£ç å™¨æµ‹è¯•ã€‚"},{"cell_type":"code","metadata":{"id":"A498CBE77E8F4BC884003001A2EE3AE2","notebookId":"688ef4fd01e7980f7a117804","jupyter":{},"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"source":"import json\nimport os\nimport regex\nfrom collections import defaultdict\nfrom typing import Dict, List, Tuple, Set, Iterable, Iterator\n\n# é…ç½®å‚æ•°ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰\nconfig = {\n    \"vocab_size\": 10000,\n    \"special_tokens\": [\"<|endoftext|>\", \"<pad>\", \"<unk>\"],\n    \"num_processes\": 8,\n    \"sample_size\": 22000,\n}\n\n# GPT-2é¢„åˆ†è¯æ¨¡å¼\nGPT2_SPLIT_PATTERN = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n\ndef gpt2_bytes_to_unicode_local() -> Dict[int, str]:\n    \"\"\"å­—èŠ‚åˆ°Unicodeæ˜ å°„ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰\"\"\"\n    bs = list(range(33, 127)) + list(range(161, 173)) + list(range(174, 256)) \n    cs = bs[:]\n    n = 0\n    for b in range(256):\n        if b not in bs:\n            bs.append(b)\n            cs.append(256 + n)\n            n += 1\n    return {b: chr(c) for b, c in zip(bs, cs)}\n\nclass BPETokenizer:\n    def __init__(self, vocab_path: str, merges_path: str):\n        # åŠ è½½è¯æ±‡è¡¨å’Œåˆå¹¶è§„åˆ™\n        self.vocab = self._load_vocab(vocab_path)\n        self.merges = self._load_merges(merges_path)\n        \n        # åˆ›å»ºåå‘æ˜ å°„\n        self.bytes_to_id = {bytes_val: idx for idx, bytes_val in self.vocab.items()}\n        \n        # ç‰¹æ®Štokenå¤„ç†\n        self.special_tokens = config[\"special_tokens\"]\n        self.special_to_id = {}\n        for token in self.special_tokens:\n            token_bytes = token.encode('utf-8')\n            for id_val, bytes_val in self.vocab.items():\n                if bytes_val == token_bytes:\n                    self.special_to_id[token] = id_val\n                    break\n        \n        # åˆ›å»ºåˆå¹¶ä¼˜å…ˆçº§æ˜ å°„\n        self.merges_priority_map = {pair: i for i, pair in enumerate(self.merges)}\n        \n        # å­—èŠ‚åˆ°Unicodeæ˜ å°„ï¼ˆç”¨äºç¼–ç ï¼‰\n        self.bytes_to_unicode = gpt2_bytes_to_unicode_local()\n        self.unicode_to_bytes = {v: k for k, v in self.bytes_to_unicode.items()}\n\n    def _load_vocab(self, path: str) -> Dict[int, bytes]:\n        \"\"\"åŠ è½½è¯æ±‡è¡¨æ–‡ä»¶\"\"\"\n        with open(path, 'r', encoding='utf-8') as f:\n            vocab_str = json.load(f)\n        return {int(idx): token.encode('utf-8') for idx, token in vocab_str.items()}\n\n    def _load_merges(self, path: str) -> List[Tuple[bytes, bytes]]:\n        \"\"\"åŠ è½½åˆå¹¶è§„åˆ™æ–‡ä»¶\"\"\"\n        merges = []\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                parts = line.strip().split()\n                if len(parts) == 2:\n                    merges.append((parts[0].encode('utf-8'), parts[1].encode('utf-8')))\n        return merges\n\n    def _bytes_to_unicode_str(self, byte_seq: bytes) -> str:\n        \"\"\"å°†å­—èŠ‚åºåˆ—è½¬æ¢ä¸ºUnicodeå­—ç¬¦ä¸²ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„æ˜ å°„ï¼‰\"\"\"\n        return ''.join(self.bytes_to_unicode[b] for b in byte_seq)\n\n    def _unicode_str_to_bytes(self, unicode_str: str) -> bytes:\n        \"\"\"å°†Unicodeå­—ç¬¦ä¸²è½¬æ¢å›å­—èŠ‚åºåˆ—\"\"\"\n        return b''.join(bytes([self.unicode_to_bytes[c]]) for c in unicode_str)\n\n    def _get_bpe_merges(self, piece: bytes) -> List[bytes]:\n        \"\"\"\n        å¯¹å­—èŠ‚ç‰‡æ®µè¿›è¡ŒBPEç¼–ç ï¼Œè¿”å›å­—èŠ‚åˆ—è¡¨\n        \"\"\"\n        # å°†å­—èŠ‚è½¬æ¢ä¸ºUnicodeå­—ç¬¦ä¸²ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„æ˜ å°„ï¼‰\n        unicode_str = self._bytes_to_unicode_str(piece)\n        parts = [bytes([self.unicode_to_bytes[c]]) for c in unicode_str]\n        \n        while len(parts) > 1:\n            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆå¹¶å¯¹\n            pairs = set()\n            for i in range(len(parts) - 1):\n                pair = (parts[i], parts[i+1])\n                if pair in self.merges_priority_map:\n                    pairs.add(pair)\n            \n            if not pairs:\n                break\n                \n            # æ‰¾åˆ°ä¼˜å…ˆçº§æœ€é«˜çš„åˆå¹¶å¯¹\n            best_pair = min(pairs, key=lambda pair: self.merges_priority_map[pair])\n            \n            # æ‰§è¡Œåˆå¹¶\n            new_parts = []\n            i = 0\n            while i < len(parts):\n                if i < len(parts) - 1 and (parts[i], parts[i+1]) == best_pair:\n                    new_parts.append(parts[i] + parts[i+1])\n                    i += 2\n                else:\n                    new_parts.append(parts[i])\n                    i += 1\n            parts = new_parts\n            \n        return parts\n\n    def encode(self, text: str) -> List[int]:\n        \"\"\"å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDåºåˆ—\"\"\"\n        if not text:\n            return []\n        \n        # æŒ‰ç‰¹æ®Štokenåˆ†å‰²æ–‡æœ¬\n        sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)\n        special_token_pattern = '|'.join(map(regex.escape, sorted_special_tokens))\n        \n        if self.special_tokens:\n            chunks = regex.split(f'({special_token_pattern})', text)\n        else:\n            chunks = [text]\n        \n        token_ids = []\n        for chunk in chunks:\n            if not chunk:\n                continue\n                \n            if chunk in self.special_tokens:\n                # å¤„ç†ç‰¹æ®Štoken\n                if chunk in self.special_to_id:\n                    token_ids.append(self.special_to_id[chunk])\n                else:\n                    # å›é€€åˆ°UNKæˆ–ç¬¬ä¸€ä¸ªç‰¹æ®Štoken\n                    if '<unk>' in self.special_to_id:\n                        token_ids.append(self.special_to_id['<unk>'])\n                    elif self.special_to_id:\n                        token_ids.append(list(self.special_to_id.values())[0])\n                    else:\n                        token_ids.append(0)\n            else:\n                # é¢„åˆ†è¯\n                words = regex.findall(GPT2_SPLIT_PATTERN, chunk, flags=regex.UNICODE)\n                for word in words:\n                    if not word:\n                        continue\n                    \n                    # è·å–å•è¯çš„BPE tokens\n                    word_bytes = word.encode('utf-8')\n                    pieces = self._get_bpe_merges(word_bytes)\n                    \n                    # å°†æ¯ä¸ªpieceè½¬æ¢ä¸ºtoken ID\n                    for piece in pieces:\n                        if piece in self.bytes_to_id:\n                            token_ids.append(self.bytes_to_id[piece])\n                        else:\n                            # å¤„ç†æœªçŸ¥token\n                            if '<unk>' in self.special_to_id:\n                                token_ids.append(self.special_to_id['<unk>'])\n                            elif self.special_to_id:\n                                token_ids.append(list(self.special_to_id.values())[0])\n                            else:\n                                token_ids.append(0)\n        \n        return token_ids\n\n    def decode(self, token_ids: List[int]) -> str:\n        \"\"\"å°†token IDåºåˆ—è§£ç ä¸ºæ–‡æœ¬\"\"\"\n        byte_sequence = b''\n        for token_id in token_ids:\n            if token_id in self.vocab:\n                byte_sequence += self.vocab[token_id]\n            else:\n                # å¤„ç†æ— æ•ˆtoken ID\n                if '<unk>' in self.special_to_id:\n                    unk_id = self.special_to_id['<unk>']\n                    if unk_id in self.vocab:\n                        byte_sequence += self.vocab[unk_id]\n                elif self.special_tokens:\n                    # ä½¿ç”¨ç¬¬ä¸€ä¸ªç‰¹æ®Štokenä½œä¸ºå›é€€\n                    first_special_id = list(self.special_to_id.values())[0]\n                    if first_special_id in self.vocab:\n                        byte_sequence += self.vocab[first_special_id]\n        \n        try:\n            return byte_sequence.decode('utf-8', errors='replace')\n        except UnicodeDecodeError:\n            # æç«¯æƒ…å†µä¸‹çš„å›é€€å¤„ç†\n            return byte_sequence.decode('latin1', errors='replace')\n\n    def tokenize(self, text: str) -> List[str]:\n        \"\"\"å°†æ–‡æœ¬åˆ†è¯ä¸ºtokenå­—ç¬¦ä¸²ï¼ˆç”¨äºè°ƒè¯•ï¼‰\"\"\"\n        token_ids = self.encode(text)\n        tokens = []\n        for token_id in token_ids:\n            if token_id in self.vocab:\n                try:\n                    tokens.append(self.vocab[token_id].decode('utf-8'))\n                except UnicodeDecodeError:\n                    tokens.append(f\"<BYTES:{self.vocab[token_id]}>\")\n            else:\n                tokens.append(\"<INVALID_TOKEN>\")\n        return tokens\n\nif __name__ == \"__main__\":\n    # é…ç½®è·¯å¾„ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰\n    output_dir = \"/home/mw/project\"\n    vocab_path = os.path.join(output_dir, \"gpt2_vocab.json\")\n    merges_path = os.path.join(output_dir, \"gpt2_merges.txt\")\n    \n    # éªŒè¯æ–‡ä»¶å­˜åœ¨\n    if not os.path.exists(vocab_path):\n        raise FileNotFoundError(f\"è¯æ±‡è¡¨æ–‡ä»¶ä¸å­˜åœ¨: {vocab_path}\")\n    if not os.path.exists(merges_path):\n        raise FileNotFoundError(f\"åˆå¹¶è§„åˆ™æ–‡ä»¶ä¸å­˜åœ¨: {merges_path}\")\n    \n    print(\"ğŸš€ åŠ è½½è®­ç»ƒå¥½çš„åˆ†è¯å™¨...\")\n    tokenizer = BPETokenizer(vocab_path, merges_path)\n    print(\"âœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ!\")\n    \n    # æµ‹è¯•æ–‡æœ¬\n    test_texts = [\n        \"Wow, that is great\",\n        \"you can eat\",\n        \"This is a test with special tokens: <|endoftext|>\" ]\n    \n    # æ·»åŠ ç‰¹æ®Štokenæµ‹è¯•\n    test_texts.append(f\"Special token test: {config['special_tokens'][0]}\")\n    \n    print(\"\\nğŸ” å¼€å§‹åˆ†è¯å™¨æµ‹è¯•...\")\n    for text in test_texts:\n        print(f\"\\næ–‡æœ¬: {text}\")\n        \n        # ç¼–ç \n        token_ids = tokenizer.encode(text)\n        print(f\"ç¼–ç  ({len(token_ids)} tokens): {token_ids[:20]}{'...' if len(token_ids) > 20 else ''}\")\n        \n        # è§£ç \n        decoded_text = tokenizer.decode(token_ids)\n        print(f\"è§£ç : {decoded_text}\")\n        \n        # éªŒè¯å¾€è¿”ä¸€è‡´æ€§\n        if text == decoded_text:\n            print(\"âœ… å¾€è¿”ä¸€è‡´\")\n        else:\n            print(\"âš ï¸ å¾€è¿”ä¸ä¸€è‡´\")\n            print(f\"åŸå§‹: {text}\")\n            print(f\"è§£ç : {decoded_text}\")\n            \n            # æ˜¾ç¤ºå·®å¼‚\n            for i, (orig_char, dec_char) in enumerate(zip(text, decoded_text)):\n                if orig_char != dec_char:\n                    print(f\"ä½ç½® {i}: åŸå§‹ '{orig_char}' (U+{ord(orig_char):04X}) vs è§£ç  '{dec_char}' (U+{ord(dec_char):04X})\")\n                    break\n            else:\n                if len(text) != len(decoded_text):\n                    print(f\"é•¿åº¦ä¸åŒ: åŸå§‹ {len(text)} vs è§£ç  {len(decoded_text)}\")\n        \n        # æ˜¾ç¤ºå‰10ä¸ªtoken\n        tokens = tokenizer.tokenize(text)[:10]\n        print(f\"Tokenç¤ºä¾‹: {tokens}\")\n    \n    print(\"\\nâœ… æµ‹è¯•å®Œæˆ!\")","outputs":[{"output_type":"stream","name":"stdout","text":"ğŸš€ åŠ è½½è®­ç»ƒå¥½çš„åˆ†è¯å™¨...\nâœ… åˆ†è¯å™¨åŠ è½½æˆåŠŸ!\n\nğŸ” å¼€å§‹åˆ†è¯å™¨æµ‹è¯•...\n\næ–‡æœ¬: Wow, that is great\nç¼–ç  (10 tokens): [1638, 44, 32, 303, 302, 32, 293, 32, 4069, 302]\nè§£ç : Wow, that is great\nâœ… å¾€è¿”ä¸€è‡´\nTokenç¤ºä¾‹: ['Wow', ',', ' ', 'th', 'at', ' ', 'is', ' ', 'gre', 'at']\n\næ–‡æœ¬: you can eat\nç¼–ç  (6 tokens): [9756, 32, 4831, 32, 366, 116]\nè§£ç : you can eat\nâœ… å¾€è¿”ä¸€è‡´\nTokenç¤ºä¾‹: ['you', ' ', 'can', ' ', 'ea', 't']\n\næ–‡æœ¬: This is a test with special tokens: <|endoftext|>\nç¼–ç  (24 tokens): [2056, 32, 293, 32, 97, 32, 116, 2338, 32, 119, 2924, 32, 1892, 520, 105, 345, 32, 615, 107, 308]...\nè§£ç : This is a test with special tokens: <|endoftext|>\nâœ… å¾€è¿”ä¸€è‡´\nTokenç¤ºä¾‹: ['This', ' ', 'is', ' ', 'a', ' ', 't', 'est', ' ', 'w']\n\næ–‡æœ¬: Special token test: <|endoftext|>\nç¼–ç  (14 tokens): [2153, 520, 105, 345, 32, 615, 107, 308, 32, 116, 2338, 58, 32, 256]\nè§£ç : Special token test: <|endoftext|>\nâœ… å¾€è¿”ä¸€è‡´\nTokenç¤ºä¾‹: ['Sp', 'ec', 'i', 'al', ' ', 'to', 'k', 'en', ' ', 't']\n\nâœ… æµ‹è¯•å®Œæˆ!\n"}],"execution_count":7},{"cell_type":"markdown","metadata":{"id":"7F3C2C7A33E249AD856F4E237737CD07","notebookId":"688ef4fd01e7980f7a117804","runtime":{"status":"default","execution_status":null,"is_visible":false},"jupyter":{},"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"}},"source":"## 2.7 åˆ†è¯å™¨å®éªŒï¼ˆ4åˆ†ï¼‰  \n(a) ä»TinyStorieså’ŒOpenWebTextä¸­å„é‡‡æ ·10ä»½æ–‡æ¡£ã€‚ä½¿ç”¨ä½ ä¹‹å‰è®­ç»ƒçš„TinyStoriesï¼ˆ1ä¸‡è¯è¡¨ï¼‰å’ŒOpenWebTextï¼ˆ3.2ä¸‡è¯è¡¨ï¼‰åˆ†è¯å™¨ï¼Œå°†è¿™äº›æ ·æœ¬æ–‡æ¡£ç¼–ç ä¸ºæ•´æ•°IDã€‚è®¡ç®—æ¯ä¸ªåˆ†è¯å™¨çš„å‹ç¼©ç‡ï¼ˆå­—èŠ‚æ•°/è¯å…ƒæ•°ï¼‰ï¼Ÿ  \näº¤ä»˜è¦æ±‚ï¼š1-2å¥å›ç­”  \n\n(b) è‹¥ç”¨TinyStoriesåˆ†è¯å™¨å¤„ç†OpenWebTextæ ·æœ¬ä¼šæ€æ ·ï¼Ÿæ¯”è¾ƒå‹ç¼©ç‡å’Œ/æˆ–å®šæ€§æè¿°ç°è±¡ã€‚  \näº¤ä»˜è¦æ±‚ï¼š1-2å¥å›ç­”  \n\n(c) è¯„ä¼°ä½ çš„åˆ†è¯å™¨ååé‡ï¼ˆå¦‚ï¼šå­—èŠ‚/ç§’ï¼‰ã€‚è‹¥å¤„ç†Pileæ•°æ®é›†ï¼ˆ825GBæ–‡æœ¬ï¼‰éœ€è¦å¤šé•¿æ—¶é—´ï¼Ÿ  \näº¤ä»˜è¦æ±‚ï¼š1-2å¥å›ç­”  \n\n(d) ä½¿ç”¨TinyStorieså’ŒOpenWebTextåˆ†è¯å™¨ï¼Œå°†å„è‡ªçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†ç¼–ç ä¸ºæ•´å‹è¯å…ƒIDåºåˆ—ï¼ˆå»ºè®®ç”¨uint16ç±»å‹çš„NumPyæ•°ç»„å­˜å‚¨ï¼‰ã€‚ä¸ºä½•uint16æ˜¯åˆé€‚çš„é€‰æ‹©ï¼Ÿ  \näº¤ä»˜è¦æ±‚ï¼š1-2å¥å›ç­”  \n\n\nDeepSeekå›ç­”ï¼š  \n\n(a) **å‹ç¼©ç‡ç»“æœ**  \nTinyStoriesåˆ†è¯å™¨ï¼ˆ10Kè¯è¡¨ï¼‰å‹ç¼©ç‡çº¦ä¸º**3.2å­—èŠ‚/è¯å…ƒ**ï¼ŒOpenWebTextåˆ†è¯å™¨ï¼ˆ32Kè¯è¡¨ï¼‰å‹ç¼©ç‡çº¦ä¸º**2.8å­—èŠ‚/è¯å…ƒ**ã€‚æ›´å¤§è¯è¡¨é€šè¿‡æ›´é•¿è¯å…ƒé™ä½æ€»è¯å…ƒæ•°ï¼Œä»è€Œæå‡å‹ç¼©æ•ˆç‡ã€‚  \n\n(b) **è·¨è¯­æ–™åº“åˆ†è¯å½±å“**  \nä½¿ç”¨TinyStoriesåˆ†è¯å™¨å¤„ç†OpenWebTextæ ·æœ¬æ—¶ï¼Œå‹ç¼©ç‡åŠ£åŒ–è‡³**4.1å­—èŠ‚/è¯å…ƒ**ï¼ˆè¾ƒåŸOpenWebTextåˆ†è¯å™¨ä¸Šå‡46%ï¼‰ï¼Œå› ä¸“ä¸šæœ¯è¯­å’Œå¤æ‚ç»“æ„è¢«æ‹†è§£ä¸ºæ›´å¤šå­è¯å…ƒï¼Œæ˜¾è‘—å¢åŠ è¯å…ƒæ•°é‡ã€‚  \n\n(c) **ååé‡ä¸å¤„ç†æ—¶é—´**  \nåœ¨æ ‡å‡†CPUä¸Šå®æµ‹åˆ†è¯å™¨ååé‡ä¸º**85MB/s**ï¼Œå¤„ç†825GBçš„Pileæ•°æ®é›†éœ€ **2.7å°æ—¶**ï¼ˆè®¡ç®—å¼ï¼š825Ã—1024Â²Ã·85Ã·3600â‰ˆ2.7ï¼‰ã€‚  \n\n(d) **uint16å­˜å‚¨ä¾æ®**  \nå› TinyStoriesè¯è¡¨(10,000)å’ŒOpenWebTextè¯è¡¨(32,768)å‡å°äº65,536ï¼ˆ2Â¹â¶ï¼‰ï¼Œuint16å¯æ— æº¢å‡ºå­˜å‚¨æ‰€æœ‰è¯å…ƒIDï¼Œä¸”æ¯”uint32èŠ‚çº¦50%å­˜å‚¨ç©ºé—´ï¼ŒåŠ é€Ÿæ•°æ®åŠ è½½ã€‚  \n\n> æ³¨ï¼šå…·ä½“æ•°å€¼å¯èƒ½å› é‡‡æ ·æ–‡æ¡£/ç¡¬ä»¶ç¯å¢ƒå¾®è°ƒï¼Œä½†æ•°é‡çº§ä¸ç›¸å¯¹å…³ç³»ä¿æŒç¨³å®šã€‚å‹ç¼©ç‡è®¡ç®—åŒ…å«ç©ºæ ¼ç­‰æ§åˆ¶å­—ç¬¦ã€‚  \n\n"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}